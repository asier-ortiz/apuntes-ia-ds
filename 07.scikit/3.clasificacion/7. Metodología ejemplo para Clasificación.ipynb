{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CUADERNO DE TRABAJO PARA CLASIFICACIÓN\n",
    "\n",
    "Vamos a trabajar y comprender cada tarea que compone un problema de modelado predictivo de aprendizaje automático:\n",
    "1. Carga de fuentes de datos\n",
    "2. Comprensión de los datos mediante:\n",
    "    1. Estadística descriptiva\n",
    "    2. Visualización.\n",
    "3. Preparación del modelo.\n",
    "    1. Pre-procesar los datos para describir mejor la estructura del problema.\n",
    "    2. Selección de características.\n",
    "4. Evaluación de los algoritmos. \n",
    "    1. Remuestreo (para evaluar el rendimiento)\n",
    "    2. Métricas\n",
    "    3. Comprobación puntual de una serie de algoritmos utilizando su propio conjunto de prueba.\n",
    "    4. Comparación y selección de modelos\n",
    "5. Mejora de la precisión del modelo\n",
    "    1. Mejora de resultados utilizando métodos de conjuntos - Ensembles.\n",
    "    2. Mediante el ajuste de parámetros de algoritmos - Tuning.\n",
    "6. Cerrar el modelo y lo dejamos disponible para su uso futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lo primero que aprenderemos es a cargar datos. El formato más común para los datos de aprendizaje automático son los archivos CSV. \n",
    "Hay varias formas de cargar un archivo CSV en Python. Podemos ver 3 formas de cargar los datos:\n",
    "1. Cargando archivos CSV con la biblioteca estándar de Python.\n",
    "2. Cargando archivos CSV con NumPy.\n",
    "3. Cargando archivos CSV con Pandas.\n",
    "\n",
    "La **API de Python** proporciona el módulo CSV y la función reader() que se pueden usar para cargar archivos CSV. Una vez cargado, puede convertir los datos CSV a una matriz NumPy y usarlos para aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Carga de CSV utilizando la librería estandar de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "fichero = './pima-indians-diabetes.data.csv'\n",
    "raw_datos_sp = open(fichero, 'rt')\n",
    "reader = csv.reader(raw_datos_sp, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "x = list(reader)\n",
    "datos = numpy.array(x).astype('float')\n",
    "print(datos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.Puedes cargar sus datos CSV usando NumPy y la función numpy.loadtxt(). \n",
    "Esta función no asume ninguna fila de encabezado y todos los datos tienen el mismo formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "\n",
    "fichero = './pima-indians-diabetes.data.csv'\n",
    "datos_np = open(fichero, 'rt')\n",
    "datos = loadtxt(datos_np , delimiter=\",\") \n",
    "print(datos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.Carga de CSV desde URL utilizando Pandas y la función pandas.read_csv():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datos=pd.read_csv('./pima-indians-diabetes.data.csv', header=None) \n",
    "datos.head()\n",
    "\n",
    "# Ponemos los nombres a las columnas basándonos en la web citada\n",
    "datos.columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigree','Age','Outcome'] \n",
    "datos.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Vemos qué datatypes ha adjudicado por defecto por si tuviéramos que modificarlos\n",
    "datos.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Para averiguar el rango de valores que toma cada atributo usamos el comando .describe \n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Vemos que no hay valores faltantes en ninguna columna\n",
    "datos.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Comprensión\n",
    "## 2.1 Analítica Descriptiva\n",
    "Este paso nos permite prepararnos para conocer y comprender los datos. Veamos 7 pasos interesantes para comprender mejor nuestros datos de aprendizaje automático con Python. Los pasos son:\n",
    "1. Echar un vistazo a sus datos en bruto (sp - sin procesar).\n",
    "2. Revisar las dimensiones del dataset.\n",
    "3. Revisar los tipos de datos de los atributos en sus datos.\n",
    "4. Resumir la distribución de instancias entre clases en nuestro dataset.\n",
    "5. Resumir nuestros datos utilizando estadísticas descriptivas.\n",
    "6. Comprender las relaciones en nuestros datos utilizando correlaciones.\n",
    "7. Revisar el sesgo de las distribuciones de cada atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 1. Echar un vistazo a sus datos en bruto (sp - sin procesar). \n",
    "Nada como ver los datos en bruto. Nos va a revelar información que no se puede obtener de otra manera. También nos permite dejar muestras que pueden convertirse en ideas sobre cómo pre-procesar y manejar los datos mejor en nuestras tareas de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " # 1. Echamos un vistazo a las 20 primeras filas\n",
    "    \n",
    "from pandas import read_csv\n",
    "\n",
    "fichero = './pima-indians-diabetes.data.csv'\n",
    "nombres = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "datos = read_csv(fichero, names=nombres)\n",
    "vistazo = datos.head(20)\n",
    "print(vistazo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** Podemos ver que la primera columna enumera el número de fila, lo cual es útil para hacer referencia a una observación específica a posteriori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 2. Revisar las dimensiones del dataset. \n",
    "Conocer en detalle la cantidad de datos que tenemos (filas y columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.Dimensiones de nuestros datos (shape)\n",
    "shape = datos.shape \n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> **Comprensión:**\n",
    "- **Demasiadas filas y algoritmos** pueden tardar demasiado en entrenarse. Muy pocas y quizás no tengamos suficientes datos para entrenar los algoritmos.\n",
    "- **Demasiados atributos y pocos algoritmos** pueden tener bajo rendimiento debido al problema de la dimensionalidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 3. Revisar los tipos de datos de los atributos en sus datos. \n",
    "Usando simplemente dtypes dentro del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3.Tipos de datos por cada atributo\n",
    "tipos = datos.dtypes \n",
    "print(tipos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** Podemos ver que la mayoría de los atributos son enteros, y que la mass y pedi son tipos de puntos flotantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 4. Resumir la distribución de instancias entre clases en nuestro dataset (para modelos de clasificación). \n",
    "En los problemas de clasificación, debemos saber cómo de equilibrados están los valores de clase. Con comunes los problemas muy desequilibrados (muchas más observaciones para una clase que para otra) y pueden necesitar de un tratamiento especial en la etapa de preparación de datos del proyecto. \n",
    "Con Pandas se puede obtener rápidamente una idea de la distribución del atributo de clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.Resumir la distribución de instancias entre clases en nuestro dataset # (para modelos de clasificación)\n",
    "cuenta_clases = datos.groupby('class').size()\n",
    "print(cuenta_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** Podemos ver que hay casi el doble de observaciones con clase 0 (sin aparición de diabetes) que con clase 1 (aparición de diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 5. Resumir nuestros datos utilizando estadísticas descriptivas. \n",
    "Es verdad que podemos crear más resúmenes de los que podemos revisar, pero la función describe() de los DataFrame enumera 8 estadísticas de cada atributo: Recuento, Media, Desviación estándar, Valor mínimo, Percentil 25, Percentil 50 (mediana), Percentil 75 y Valor máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " # 5.Resumen estadística descriptiva\n",
    "from pandas import set_option\n",
    "set_option('display.width', 100) \n",
    "set_option('precision', 3) \n",
    "descripcion = datos.describe() \n",
    "print(descripcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** Al describir nuestros datos de esta manera, tenemos posibilidad de revisar las observaciones de los resultados en detalle. Esto es útil también para revisar la presencia de valores de NA para datos faltantes o distribuciones llamativas para atributos.\n",
    "- *Hay un par de llamadas a la opción pandas.set() que nos permite cambiar la precisión de los números y la amplitud de la salida, pero lo hacemos para que sea más legible para este ejemplo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 6. Comprender las relaciones en nuestros datos utilizando correlaciones (relación entre dos variables y su variabilidad conjunta). \n",
    "El método más común para calcular la correlación es el coeficiente de correlación de Pearson, que asume una distribución normal de los atributos involucrados:\n",
    "- Una correlación de -1 o 1 muestra una correlación negativa o positiva completa, respectivamente. \n",
    "- Mientras que un valor de 0 no muestra ninguna correlación en absoluto. \n",
    "\n",
    ">Algunos algoritmos de aprendizaje automático (la regresión lineal y logística), pueden tener un rendimiento deficiente si existen atributos muy correlacionados en nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " # 6.Parejas de correlaciones de Pearson\n",
    "correlaciones = datos.corr(method='pearson') \n",
    "print(correlaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** La matriz enumera todos los atributos en la parte superior y en el lateral, para dar una correlación entre todos los pares de atributos (dos veces, porque la matriz es simétrica). Puede ver la línea diagonal a través de la matriz desde la esquina superior izquierda hasta la esquina inferior derecha de la matriz que muestra la correlación perfecta de cada atributo consigo mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 7. Revisar el sesgo de las distribuciones de cada atributo (distribuciones univariadas). \n",
    "**El sesgo (skew)** se refiere a una distribución que se supone gaussiana (normal o curva de campana) que se desplaza o aplasta en una dirección u otra. Muchos algoritmos de aprendizaje automático asumen una distribución gaussiana. \n",
    "\n",
    ">Saber que un atributo tiene un sesgo nos permitirá realizar una preparación de datos que tenga en cuenta esto y corregirlo para mejorar la precisión de nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 7.Sesgo por cada atributo\n",
    "skew = datos.skew() \n",
    "print(skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**Comprensión:** El resultado de sesgo muestra un sesgo positivo (derecha) o negativo (izquierda). Los valores más cercanos a cero muestran menos sesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2. Visualización\n",
    "La forma más rápida de aprender más sobre sus datos es usar la visualización de datos. Está orientada a comprender sus datos para obtener los mejores resultados de los algoritmos de aprendizaje automático.\n",
    "\n",
    "Se suele trabajar con diagramas univariadas que se utilizan para comprender cada atributo de nuestro dataset de forma independiente:\n",
    "- Histogramas.\n",
    "- Diagramas de densidad.\n",
    "- Diagramas de caja y bigotes (Box & Whisker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 1. Diagramas univariados. Histogramas\n",
    "Los histogramas agrupan los datos en **contenedores (bins)** y le proporcionan un recuento del número de observaciones en cada contenedor.\n",
    "\n",
    "De la forma de los contenedores, puede obtener rápidamente una idea de si un atributo es **gaussiano, sesgado** o incluso tiene una distribución **exponencial**. También puede ayudarte a ver posibles valores atípicos (**outliers**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Histogramas univariados\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "\n",
    "fichero = './pima-indians-diabetes.data.csv'\n",
    "nombres = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "datos = read_csv(fichero, names=nombres)\n",
    "\n",
    "datos.hist()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 2. Diagramas univariados. Densidad\n",
    "Los gráficos de densidad son otra forma de obtener una idea rápida de la distribución de cada atributo. Las parcelas se ven como un histograma abstracto con una curva suave dibujada a través de la parte superior de cada contenedor (bin), como lo veíamos con los histogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Diagramas de densidad univariada\n",
    "\n",
    "datos.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 3. Diagramas univariados.  Caja y bigotes (Box & Whisker)\n",
    "Otra forma útil de revisar la distribución de cada atributo es usar los diagramas de caja y bigotes o los gráficos de caja para abreviar.\n",
    "\n",
    ">Los diagramas de caja resumen la distribución de cada atributo, trazando una línea para la mediana (valor medio) y un cuadro alrededor de los percentiles 25 y 75 (el 50% medio de los datos).\n",
    "\n",
    ">Los **bigotes** dan una idea de la dispersión de los datos y los puntos fuera de los bigotes muestran valores de valores atípicos candidatos (valores que son 1.5 veces mayores que el tamaño de la dispersión del 50% medio de los datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Diagrama de caja y bigotes\n",
    "\n",
    "datos.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 4. Diagramas Multivariados. Matriz de Correlación\n",
    "La correlación da una indicación de cómo de relacionados están los cambios entre dos variables:\n",
    "- Si dos variables cambian en la misma dirección, se correlacionan positivamente. \n",
    "- Si cambian juntos en direcciones opuestas (uno sube, uno baja), entonces están correlacionados negativamente.\n",
    "\n",
    ">Puedes calcular la correlación *entre cada par de atributos*. Esto se llama una matriz de correlación. Luego puede trazar la matriz de correlación y tener una idea de qué variables tienen una alta correlación entre sí.\n",
    "\n",
    ">Está muy bien saberlo, ya que algunos algoritmos de aprendizaje automático, como la regresión lineal y logística, pueden tener un bajo rendimiento si hay variables de entrada altamente correlacionadas en sus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#1.Diagrama de la matriz de correlación (multivariante)\n",
    "import numpy as np\n",
    "\n",
    "correlaciones = datos.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dibujamos la matriz de correlación (genérica)\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlaciones, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dibujamos la matriz de correlación\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlaciones, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(nombres)\n",
    "ax.set_yticklabels(nombres)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 5. Matriz de dispersión\n",
    "Un diagrama de dispersión muestra la relación entre dos variables como puntos en **2 dimensiones**, un eje para cada atributo. Puede crear un diagrama de dispersión para cada par de atributos en sus datos.\n",
    "\n",
    ">Dibujar todos estos gráficos de dispersión juntos se llama una **matriz de gráficos de dispersión**.\n",
    "\n",
    ">Los diagramas de dispersión son útiles para detectar relaciones estructuradas entre variables, como un resumen de la relación entre dos variables con una línea. Los atributos con relaciones estructuradas también pueden estar correlacionados y ser buenos candidatos para ser eliminados de su conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(datos)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# 3. Preparación del modelo\n",
    "## 3.1 Pre-procesado\n",
    "Muchos algoritmos de aprendizaje automático hacen suposiciones sobre sus datos. A menudo es una muy buena idea preparar sus datos de tal manera que exponga mejor la estructura del problema a los algoritmos de aprendizaje automático que pretende utilizar.\n",
    "\n",
    "Con la ayuda de scikit-learn podemos:\n",
    "- **a) Re-escalar los datos.**\n",
    "- **b) Estandarizar los datos.**\n",
    "- **c) Normalizar los datos.**\n",
    "- **d) Binarizar datos.**\n",
    "\n",
    "El objetivo de transformar los datos es el de exponer mejor la estructura del problema a los algoritmos de modelado.\n",
    "\n",
    "**NECESIDAD:** \n",
    "- A veces los algoritmos pueden ofrecer mejores resultados sin procesamiento previo.\n",
    "- Lo más normal:\n",
    "    - Crear muchas vistas y transformaciones diferentes de sus datos.\n",
    "    - Ejecutar un grupo de algoritmos en cada vista de su conjunto de datos.\n",
    "    \n",
    "La biblioteca scikit-learn proporciona dos formas estándar para transformar datos. Cada uno es útil en diferentes circunstancias. Estas transformaciones se calculan de tal manera que se pueden usar con datos train y con cualquier muestra de datos que pueda tener en el futuro.\n",
    "\n",
    "Con scikit-learn trabajaremos 2 métodos distintos:\n",
    "- **Ajuste y transformación múltiple:**\n",
    "    - Llamamos a la función *fit()* para preparar los parámetros de la transformación una vez en sus datos.\n",
    "    - Más adelante, utilizamos utilizar la función *transform()* en los mismos datos para prepararla para el modelado y nuevamente en el conjunto de datos de prueba o validación o en los nuevos datos que pueda ver en el futuro. \n",
    "- **Combinación de ajuste y transformación:**\n",
    "    - Utilizado para una sola tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos el dataset con Pandas que es muy útil\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "\n",
    "fichero = './pima-indians-diabetes.data.csv'\n",
    "nombres = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(fichero, names=nombres)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**a) Re-escalar de datos (entre 0 y 1)**\n",
    "Cuando sus datos se componen de atributos con diferentes escalas, muchos algoritmos de aprendizaje automático pueden beneficiarse de volver a escalar los atributos para que todos tengan la misma escala\n",
    "\n",
    ">Esto se conoce como normalización y los atributos a menudo se vuelven a escalar en el rango entre **0 y 1**:\n",
    "- Esto es útil para los algoritmos de optimización utilizados en el núcleo de los algoritmos de aprendizaje automático, como la pendiente de gradiente (gradient descent).\n",
    "- También es útil para algoritmos que ponderan entradas como regresión y redes neuronales y algoritmos que usan medidas de distancia como k-vecinos más cercanos (k-Nearest Neighbors). \n",
    "\n",
    ">Puedes volver a escalar sus datos con scikit-learn usando la clase **MinMaxScaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# separamos matriz en componentes de entrada y salida\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# resumimos los datos transformados\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**b) Estandarizar los datos (0 mean, 1 stdev).**\n",
    "La estandarización es una técnica útil para transformar atributos con una distribución gaussiana y medias diferenciales y desviaciones estándar a una distribución gaussiana estándar con una media de 0 y una desviación estándar de 1.\n",
    "\n",
    ">Es más adecuada para técnicas que asumen una distribución gaussiana en las variables de entrada y trabajar mejor con datos re-escalados, como regresión lineal, regresión logística y análisis de discriminación lineal.\n",
    "\n",
    ">Puede estandarizar los datos usando scikit-learn con la clase **StandardScaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# matriz separada en componentes de entrada y salida\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "# resumimos los datos transformados\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**c) Normalizar datos (longitud de 1).**\n",
    "Normalizar en scikit-learn se refiere a cambiar la escala de cada **observación (fila)** para que tenga una longitud de 1 (denominada norma de unidad o vector con la longitud de 1 en álgebra lineal).\n",
    "\n",
    ">Este método de preprocesamiento puede ser útil para **conjuntos de datos dispersos** (muchos ceros) con atributos de diferentes escalas cuando se usan algoritmos que ponderan valores de entrada como redes neuronales y algoritmos que usan medidas de distancia como k-Vecinos más cercanos.\n",
    "\n",
    ">Podemos normalizar los datos en Python con scikit-learn usando la clase **Normalizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# separamos matriz en componentes de entrada y salida\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "# resumimos los datos transformados\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">**d) Binarización (0,1).**\n",
    "Podemos transformar sus datos utilizando un umbral binario.\n",
    "Todos los valores por encima del umbral están marcados como 1 y todos iguales o por debajo están marcados como 0. Esto se denomina ‘binarizar’ sus datos o **poner un umbral** a sus datos:\n",
    "- Puede ser útil cuando tienes probabilidades de que quieras hacer valores nítidos.\n",
    "- También es útil cuando se trata de ingeniería de características y desea agregar nuevas características que indiquen algo significativo.\n",
    "\n",
    "Puede crear nuevos atributos binarios en Python usando scikit-learn con la clase **Binarizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "\n",
    "# resumimos los datos transformados\n",
    "set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Selección de Características\n",
    "\n",
    "Las características de los datos utilizados para entrenar los modelos de aprendizaje automático tienen mucha influencia en el rendimiento que darán.\n",
    "\n",
    "Las características irrelevantes o parcialmente relevantes pueden afectar negativamente el rendimiento del modelo. \n",
    "\n",
    "Veamos técnicas de selección automática de características que puede utilizar para preparar sus datos de aprendizaje automático en Python con scikit-learn, destacando:\n",
    "1. **Selección univariante.**\n",
    "2. **Eliminación de la característica recursiva.**\n",
    "3. **Análisis de componentes principales.**\n",
    "4. **Importancia de la característica.**\n",
    "\n",
    "**OBJETIVO:** La selección de características es un proceso en el que selecciona automáticamente aquellas características en nuestros datos que afectan más a la variable o resultado de predicción que buscamos.\n",
    "\n",
    "Tener características irrelevantes en los datos puede disminuir la precisión de muchos modelos, especialmente algoritmos lineales como regresión lineal y logística.\n",
    "\n",
    "Tres beneficios de realizar la selección de características antes de modelar sus datos son:\n",
    "- Reduce el **OVERFITTING** (sobreajuste): menos datos redundantes significa reducir las probabilidades de **tomar decisiones basadas en el ruido**.\n",
    "- Mejora del **ACCURACY** (precisión): menos datos engañosos significa SIEMPRE una mejora del de la precisión del modelado.\n",
    "- Reduce el tiempo de **TRAINING** (entrenamiento): Menos datos significa que los algoritmos se entrenan más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Para Selección univariada\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "## Para PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## Para RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "## Para importancia de la característica\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 1. Selección univariante.\n",
    "Jugar probando con estadísticas nos ayuda a seleccionar aquellas características que tienen una relación más fuerte con la variable de salida. La biblioteca scikit-learn proporciona la clase SelectKBest que se puede usar con diferentes pruebas estadísticas para seleccionar un número específico de funciones.\n",
    "\n",
    ">En el ejemplo utiliza la prueba estadística de ji cuadrado (chi2) para características no negativas para seleccionar 4 de las mejores características del dataset Pima (diabetes en indios Pima).\n",
    "\n",
    ">**CONCLUSIÓN:** Puede ver los puntajes de cada atributo y los 4 atributos elegidos (aquellos con los puntajes más altos): **plas, test, mass y age.** Obtenemos los nombres de los atributos elegidos asignando manualmente el índice de las 4 puntuaciones más altas al índice de los nombres de atributos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.Extracción de características con pruebas estadísticas univariadas \n",
    "\n",
    "##  (Chi-cuadrado para clasificación)\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# resumir puntuaciones\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# resumen de las características seleccionadas\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 2. RFE (*Recursive Feature Elimination* - Eliminación Recursiva de Características)\n",
    "La Eliminación de características recursivas (o RFE) funciona eliminando atributos recursivamente y construyendo un modelo sobre aquellos atributos que permanecen. Utiliza la precisión del modelo para identificar qué atributos (y qué combinación de atributos) contribuyen más a la predicción del atributo objetivo.\n",
    "\n",
    ">En el ejemplo utilizamos RFE con el algoritmo de regresión logística para seleccionar las 3 funciones principales. La elección del algoritmo no importa demasiado siempre que sea hábil y consistente.\n",
    "\n",
    ">**CONCLUSIÓN:** Puedes ver que RFE eligió las 3 características principales como preg, mass y pedi. Estos están marcados como TRUE en la **matriz de soporte** y marcados con una opción 1 en la **matriz de clasificación**. También podemos asignar manualmente los índices de características a los índices de nombres de atributos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.Extracción de características con RFE\n",
    "## modelo = LogisticRegression(solver='lbfgs')\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "modelo = SGDClassifier(max_iter=500,tol=1e-3,loss='log') #loss='modified_huber'\n",
    "rfe = RFE(modelo, 3)\n",
    "\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(nombres)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 3. PCA (*Principal Component Analysis* – Análisis de Componente Principal)\n",
    "El análisis de componentes principales (o PCA) utiliza el álgebra lineal para transformar el conjunto de datos en una forma comprimida. Se trata de una **técnica reducción de datos**. Una **propiedad** de PCA es que puede elegir el número de dimensiones o componentes principales en el resultado transformado. \n",
    "\n",
    ">En el ejemplo, usamos PCA y seleccionamos 3 componentes principales.\n",
    "\n",
    ">**CONCLUSIÓN:** Podemos ver que el conjunto de datos transformado (3 componentes principales) se parece poco a los datos de origen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3.Extracción de características con PCA\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# resumir componentes\n",
    "print(\"Varianza explicada: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 4. Importancia de Características\n",
    "Las bolsas de árboles de decisión (bagged decisión trees) como Random Forest y Extra Trees se pueden usar para estimar la importancia de las características.\n",
    "\n",
    ">En el ejemplo, construimos un clasiﬁcador *ExtraTreesClassifier* para el inicio de datos.\n",
    "\n",
    ">**CONCLUSIÓN:** Puede ver que se nos otorga una puntuación de importancia para cada atributo en el que cuanto mayor sea ésta, más importante es el **atributo**. Las puntuaciones sugieren la importancia de los plas, age y mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.Extracción de importancia de las características\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# 4.Evaluación de los algoritmos\n",
    "\n",
    "##    4.1. Remuestreo (para evaluar el rendimiento)\n",
    "Esta etapa trata de resolver la siguiente necesidad: saber cómo se comportarán nuestros algoritmos con datos aún no trabajados.\n",
    "\n",
    "Debemos usar nuestro algoritmo entrenado con nuevos datos (no utilizados para entrenar), con lo que la **EVALUACIÓN es una estimación para comprobar como de buenas serán las predicciones que hará en la práctica.** NO es una garantía de rendimiento.\n",
    "\n",
    "**PROBLEMA PLANTEADO**\n",
    "*¿Cuál es el problema de usar un algoritmo de aprendizaje automático en un dataset de entrenamiento y después utilizar las predicciones de este mismo dataset para evaluar el rendimiento?* La respuesta es simple:\n",
    "\t\t**OVERFITTING o SOBREAJUSTE**\n",
    "Como el algoritmo se acuerda de todo lo que ha observado durante el entrenamiento y por tanto sabe la respuesta, no a nuevas preguntas, sino a las mismas de antes tendrá una puntuación perfecta con ellas. Pero cuando vaya a predecir dará resultados muy malos.\n",
    "\n",
    "Debemos usar nuestro algoritmo entrenado con nuevos datos (no utilizados para entrenar), con lo que la **EVALUACIÓN es una estimación para comprobar como de buenas serán las predicciones que hará en la práctica**. **NO** es una garantía de rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Métodos de Evaluación de algoritmos\n",
    "\n",
    "\"\"\"\n",
    "##Preparamos los entornos\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Una vez estimado el rendimiento de nuestro algoritmo, podemos volver a entrenar el algoritmo entrenado ﬁnal en todo el dataset de entrenamiento y prepararlo para el uso en operación (en real).\n",
    "\n",
    "Vamos a revisar **4 técnicas** que podemos usar para dividir nuestro conjunto de datos de entrenamiento y crear estimaciones útiles de rendimiento para nuestros algoritmos de aprendizaje automático:\n",
    "1. Conjuntos de entrenamiento y test (particionado).\n",
    "2. Validación cruzada (Cross-Validation) de k-folds.\n",
    "3. Validación cruzada dejando fuera 1 partición (LOO).\n",
    "4. Validación cruzada sobre particiones de Entrenamientos-Pruebas aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### Técnica 1. Conjuntos de entrenamiento y test (Train-Test)\n",
    "\n",
    "Es el método más simple: utilizar **diferentes** conjuntos de datos de entrenamiento y test. Cogemos el *dataset* original y lo dividimos en 2:\n",
    "- **Entrenamos** el algoritmo en la primera parte y testeamos predicciones en la segunda parte.\n",
    "- **Evaluamos** las predicciones últimas contra los resultados esperados. \n",
    "- **Tamaño de la división:** Puede depender del tamaño y las especificaciones del dataset, aunque es común utilizar **(2/3 – 1/3) -> el 67%** de los datos para la entrenar y el 33% restante para las pruebas.\n",
    "\n",
    "**VENTAJAS:** Es muy rápido. Si hay millones de registros y hay pruebas sólidas de que cada partición sigue representando el problema subyacente, es muy útil cuando con otros métodos tarda mucho tiempo en entrenarse.\n",
    "\n",
    "**INCONVENIENTES:** Mucha variación, que hace que las diferencias en precisión entre el dataset de entrenamiento y el de test puedan ser significativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar usando conjuntos de datos Train-Test con regresión logística\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Importante mantener la misma semilla para comparar entre distintas técnicas.\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "# NOS FIJAMOS EN DETALLE - ver cómo se entrenan los datos para hacer un model.fit(), mediante train_test_split()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=150)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "results = model.score(X_test, Y_test)\n",
    "print(\"Precisión TT: %.3f%%\" % (result*100.0))\n",
    "\n",
    "## AVANZADO. Podéis utilizar funciones que nos digan el timestamp antes y después de la ejecución, asignarlas a variables y \n",
    "##  restarlas para ver el tiempo de procesamiento de cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### Técnica 2. Validación cruzada (Cross-Validation) de k-folds\n",
    "\n",
    "La **validación cruzada (o Cross-Validation)** es un enfoque que puede utilizar para estimar el rendimiento de un algoritmo de AA con menos varianza que una única división de conjunto de prueba de entrenamiento. \n",
    "- Dividimos el dataset en partes k (por ejemplo, k = 3, 5 ó 10 para miles de registros). Cada división de los datos se llama **split**. \n",
    "- El algoritmo se entrena en k −1 splits sin usar 1 y se prueba en ese split sin usar.\n",
    "- Esto se repite para que cada split del dataset en algún momento sea el no usado.\n",
    "- Como resultado tenemos k diferentes valoraciones de rendimiento que podemos resumir utilizando **una media y una desviación estándar**.\n",
    "\n",
    "La elección de k debe hacerse de forma que el tamaño de cada partición de prueba (test) sea lo suficientemente grande como para ser una muestra razonable del problema, a la vez que se permiten suficientes iteraciones de la evaluación del algoritmo (en Train-Test) que de buenos resultados con nuevos datos.\n",
    "\n",
    "**VENTAJAS:**\n",
    "Es más preciso porque el algoritmo se entrena y evalúa varias veces con  diferentes datos.\n",
    "\n",
    "**INCONVENIENTES:** \n",
    "Depende de la selección correcta de las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Evaluar con Validación Cruzada - Cross Validation\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=150)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Precisión VC: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### Técnica 3. Validación cruzada dejando fuera 1 partición (LOO)\n",
    "\n",
    "Se trata de una variación en la que podemos configurar la validación cruzada para que el tamaño del split sea 1 (k se ajusta al número de observaciones del dataset) y se denomina validación cruzada leave-one-out (LOO).\n",
    "\n",
    "**VENTAJA:** \n",
    "Como resultado nos proporciona una gran cantidad de medidas de rendimiento resumiendo el esfuerzo necesario para ofrecer estimaciones razonables de la precisión del modelo para nuevos datos.\n",
    "\n",
    "**INCONVENIENTE:** \n",
    "Depende de la selección correcta de las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar con Validación Cruzada dejando una partición - LOO\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=150)\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Precisión VC_LOO: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### Técnica 4. Validación cruzada sobre particiones de Train-Test aleatorias (*Shuffle Split*)\n",
    "\n",
    "Otra variación en la **validación cruzada de K-fold** es crear una división aleatoria de los datos como la división de Test-Train descrita anteriormente, pero **repetir el proceso de división y evaluación del algoritmo varias veces**, como la validación cruzada. \n",
    "\n",
    "**VENTAJA:**\n",
    "Combina la velocidad de usar una Test-Train con la reducción en la varianza en el rendimiento estimado de la validación cruzada de k-fold. Además podemos repetir el proceso tantas veces como sea necesario para mejorar la precisión. \n",
    "\n",
    "**INCONVENIENTE:** \n",
    "Las repeticiones pueden incluir gran parte de los mismos datos en los split  de Train-Test de una ejecución a otra, lo que introduce redundancia en la evaluación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluación mediante repetición aleatoria de particiones Train-Test\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_splits = 10\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=150)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Precisión VC_ShuffleSplit: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CONSEJOS DE REMUESTREO\n",
    "\n",
    "- Normalmente la **validación cruzada con k-fold es el estándar de oro** para evaluar el rendimiento de un algoritmo de aprendizaje automático en datos no tratados con k establecido en 3, 5 o 10.\n",
    "\n",
    "- El uso de un Train-Test es bueno para **mejorar la rapidez cuando se usa un algoritmo lento** y produce estimaciones de rendimiento con **menor sesgo** cuando se usan datasets grandes.\n",
    "\n",
    "- Técnicas como la validación cruzada dejando uno fuera (LOO) y las divisiones aleatorias repetidas (Shuffle-splits) serán aproximaciones útiles para intenta equilibrar la varianza en el **rendimiento estimado, la velocidad de entrenamiento del modelo y el tamaño del *dataset**.\n",
    "\n",
    "- El mejor consejo es experimentar y encontrar una técnica rápida para el problema y que produzca estimaciones razonables de rendimiento que me permita utilizarlas para tomar decisiones. En caso de duda, utilizad 10 veces la validación cruzada.\n",
    "\n",
    "### MAL USO DE LA VALIDACIÓN CRUZADA\n",
    "\n",
    "- Usar la validación cruzada para evaluar varios modelos, y sólo indicando los resultados para el modelo con los mejores resultados.\n",
    "- Realizar un análisis inicial para identificar las características más informativas utilizando el dataset completo, si la selección de característica o el ajuste del modelo lo requiere por el propio procedimiento de modelado, esto debe repetirse en cada conjunto de Train. Si se utiliza la VC para decidir qué características se van a utilizar, se deberá realizar un proceso interno de VC para llevar a cabo la selección de características en cada conjunto de Train.\n",
    "- Permitir que algunos de los datos de entrenamiento esté también incluido en el conjunto de Test, esto puede suceder debido a \"hermanamiento\" en el dataset, con lo que varias muestras exactamente idénticas o casi idénticas pueden estar presentes en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## 4.2. Métricas\n",
    "Las **métricas** que elegimos para evaluar los algoritmos de aprendizaje automático son muy importantes:\n",
    "- Influyen en cómo se mide y compara el rendimiento de los algoritmos de aprendizaje automático. \n",
    "- Influyen en cómo evaluaremos la importancia de las diferentes características en los resultados y con ello la elección final de qué algoritmo elegir.\n",
    "\n",
    "Hay diferentes métricas de evaluación para tipos de problemas de aprendizaje automático que tengan con **clasificación** y con **regresión**:\n",
    "- Por ejemplo, el caso que utilizamos con el dataset de los indios Pima es un problema de clasificación binaria en el que todas las variables de entrada son numéricas.\n",
    "- En el dataset  del precio de las viviendas en Boston (Boston House Price) se trabaja con un problema de regresión en el que también todas las variables de entrada son numéricas.\n",
    "\n",
    "En ambos casos evaluamos los mismos algoritmos, Regresión logística para clasificación y Regresión lineal para regresión. Se utiliza un de validación cruzada de 10-fold para demostrar cada métrica ya que es el escenario más probable que utilizaremos: \n",
    "- Utilizaremos la función cross_validation.cross_val_score(): (http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html) utilizada para informar del rendimiento en cada código de evaluación. \n",
    "- Haremos uso de diferentes métricas de puntuación y todas las puntuaciones se detallan para que puedan clasificarse en orden ascendente (la puntuación más alta es la mejor). Algunas métricas de evaluación (como el error cuadrático medio) son puntuaciones naturalmente descendentes (la puntuación más pequeña es la mejor) y, por tanto, se notifican como negativas por la función de VC.\n",
    "\n",
    "### Métricas de clasificación y métodos de conveniencia\n",
    "Los problemas de clasificación son probablemente la tipología más común de problemas de aprendizaje automático y, por eso hay una gran cantidad de métricas que se pueden usar para evaluar las predicciones de estos problemas.\n",
    "\n",
    "Trabajaremos 3 métricas de clasificación:\n",
    "1. **Precisión** (Accuracy).\n",
    "2. **Pérdida logarítmica** (Logarithmic Loss).\n",
    "3. **Área bajo la Curva ROC** (AUC ROC).\n",
    "\n",
    "Usaremos 2 métodos de conveniencia:\n",
    "4. **Matriz de confusión**.\n",
    "5. **Informe de clasiﬁcación**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# VC-Precisión de clasificación \n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOS INTRODUCIMOS EN LAS 3 MÉTRICAS\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Métricas\n",
    "> ### 1. Precisión de clasificación:\n",
    "Nº de predicciones correctas hechas como proporción de todas las predicciones hechas:\n",
    "- Es la métrica de evaluación más común y más mal utilizada para problemas de clasificación. \n",
    "- Realmente sólo es adecuada con un número igual de observaciones en cada clase (casi nunca se da) y que todas las predicciones y errores de predicción son igualmente importantes (tampoco se suele dar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Asignar a la variable scoring se valor que le corresponde para lograr precisión o accuracy\n",
    "scoring = 'accuracy'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Precisión: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 2. Pérdida logarítmica (LogLoss): \n",
    "Métrica de rendimiento para evaluar las predicciones de probabilidades de pertenencia a una clase determinada. \n",
    "- La probabilidad escalar entre 0 y 1 se puede ver como una medida de confianza para una predicción de un algoritmo. \n",
    "- Las predicciones que son correctas o incorrectas son recompensadas o castigadas proporcionalmente a la confianza de la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Asignad a la variable scoring se valor que le corresponde para lograr logloss\n",
    "scoring = 'neg_log_loss'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 3. Área bajo la curva ROC:\n",
    "Métrica de rendimiento para problemas de clasificación **binaria**. El AUC representa la capacidad de un modelo para discriminar entre clases positivas y negativas.\n",
    "- Un área de 1.0 representa un modelo que hizo perfectamente todas las predicciones. Un área de 0.5 representa un modelo que es tan bueno como aleatorio. \n",
    "- ROC se puede dividir en **sensibilidad** y **especificidad**. Un problema de clasificación binaria es realmente un intercambio entre sensibilidad y especificidad:\n",
    "    - La **sensibilidad** es el llamado **recuerdo** o la verdadera tasa positiva: Nº de instancias de la clase positiva (primera) que realmente predijo correctamente.\n",
    "    - La **especificidad** también se llama verdadera tasa negativa: Nº de instancias de la clase negativa (segunda) que en realidad se predijeron correctamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Asignad a la variable scoring se valor que le corresponde para lograr curva ROC\n",
    "scoring = 'roc_auc'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std())*100)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "probs = model.predict_proba(X_test)\n",
    "# mantenemos las probabilidades de salidas 1\n",
    "probs = probs[:, 1]\n",
    "# Calcular AUC\n",
    "auc = roc_auc_score(Y_test, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "# Calcular la curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, probs)\n",
    "\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "pyplot.plot(fpr, tpr, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "# PRIMER PASO. NOS INTRODUCIMOS EN LOS DOS MÉTODOS - entrenamos los datos para hacer un model.fit() \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Métodos de conveniencia\n",
    "\n",
    "> ### 4. Matriz de confusión: \n",
    "Es una presentación práctica de la precisión de un modelo con **2 o más clases**.\n",
    "- La tabla presenta **predicciones** sobre el eje x y **resultados de precisión** en el y. Las celdas de la tabla son el número de predicciones realizadas por un algoritmo de aprendizaje automático. \n",
    "- Por ejemplo, un algoritmo de aprendizaje automático puede predecir 0 ó 1 y cada predicción puede haber sido 0 ó 1:\n",
    "    - Las predicciones para 0 que en realidad eran 0 aparecen en la celda para predicción = 0 y real = 0,\n",
    "    - mientras que las predicciones para 0 que fueron en realidad 1 aparece en la celda para predicción = 0 y real = 1 (y así sucesivamente).\n",
    "- La mayoría de las precisiones deberían caer en la diagonal que representan las predicciones correctas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# SEGUNDO PASO. Asignamos a predicted el valor de X_test y creamos la matriz de confusión con el método confusion_matriz\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 5. Informe de clasificación: \n",
    "La biblioteca de scikit-learn proporciona un informe de adecuación cuando trabajamos en problemas de clasificación que nos proporciona una idea inicial de la precisión de un modelo utilizando una serie de medidas.\n",
    "- La función *ranking_report()* muestra la precisión, el recuerdo, el F1-score y el soporte para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TERCER PASO. Asignamos a predicted el valor de X_test y creamos el informe con classification_report\n",
    "predicted = model.predict(X_test)\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.3. Test Puntual\n",
    "El test puntual es una forma de descubrir qué algoritmos funcionan bien en un problema de aprendizaje automático. \n",
    "- Como no sabemos de antemano cuál será mejor, debemos probar varios métodos y centrarnos en aquellos que demuestran ser más precisos.\n",
    "- Debe usar el método de prueba y error para descubrir una lista breve de algoritmos que funcionan bien en su problema, que luego puede duplicar y ajustar más. \n",
    "- Se trata de ajustarnos a nuestro dataset para escoger el mejor algoritmo:\n",
    "    - Probar una mezcla de representaciones de algoritmos (árboles, instancias).\n",
    "    - Probar una mezcla de algoritmos de aprendizaje.\n",
    "    - Probar una mezcla de tipos de modelos (funciones lineales-no lineales o paramétricas-no paramétricas).\n",
    "    \n",
    "En un **problema de clasificación** nos interesa saber:\n",
    "1. Cómo hacer test de los algoritmos de aprendizaje automático.\n",
    "2. Cómo hacer test de algoritmos de clasificación lineal. Veamos **2**:\n",
    "    1. Regresión logística.\n",
    "    2. Análisis Lineal Discriminante (LDA).\n",
    "3. Cómo hacer test de algoritmos de clasificación no lineal. Veamos **4**:\n",
    "    1. k-Nearest Neighbors (o K-Vecinos más cercanos).\n",
    "    2. Naïve Bayes.\n",
    "    3. Árboles de clasificación y regresión.\n",
    "    4. Máquinas de Vector Soporte o Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### I.Algoritmos Lineales de clasificación\n",
    "> ### 1. Regresión Logística\n",
    "- La regresión logística asume una distribución gaussiana para las variables de entrada numéricas y puede modelar problemas de clasificación binaria. \n",
    "- Puedes construir un modelo de regresión logística utilizando la clase *LogisticRegression()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Para un ejemplo de clasificación vamos a hacer chequeos puntuales que nos \n",
    "permitan saber cuáles de nuestros algoritmos se van a comportar mejor a la \n",
    "hora de resolver nuestro problema\n",
    "\"\"\"\n",
    "# Proceso común a todos los escenarios\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.Clasificación mediante Regresión Logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Regresión logística: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 2. Análisis Lineal Discriminante (LDA):\n",
    "- El análisis discriminante lineal o LDA es una técnica estadística para la clasificación binaria y multiclase. También asume una distribución gaussiana para las variables de entrada numéricas. \n",
    "- Puedes construir un modelo de LDA usando la clase *LinearDiscriminantAnalysis()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.Clasificación con LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"LDA: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### II.Algoritmos No Lineales de clasificación\n",
    ">### 1. k-Nearest Neighbors (o K-Vecinos más cercanos):\n",
    "- El algoritmo KNN utiliza una métrica de distancia para encontrar las k-instancias más similares en los datos de entrenamiento de una nueva instancia y toma el resultado promedio de los vecinos como la predicción. \n",
    "- Para construir el modelo KNN usa la clase *KNeighborsClassifier()*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Clasificación No Lineal KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"KNN: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 2. Naive Bayes:\n",
    "- Naive Bayes calcula la **probabilidad** de cada clase y la **probabilidad condicional** de cada clase dado cada valor de entrada. Estas probabilidades se estiman para datos nuevos y se multiplican juntas, asumiendo que son todas independientes (un supuesto simple o ingenuo). Cuando se trabaja con datos de valor real, se supone que una distribución gaussiana para estimar fácilmente las probabilidades de las variables de entrada utilizando la función de densidad de probabilidad gaussiana (**GPDF**). \n",
    "- Para construir un modelo Naive Bayes usa la clase *GaussianNB()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Clasificación NL Naive Bayes Gaussiana\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Naive Bayes: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 3. Árboles de clasificación y regresión:\n",
    "- Los árboles de clasificación y regresión (**CART** o simplemente árboles de decisión) construyen un árbol binario a partir de los datos de entrenamiento. Los puntos de división se eligen de forma rápida al evaluar cada atributo y cada valor de cada atributo en los datos de entrenamiento para minimizar una función de coste (como el índice de Gini).\n",
    "- Puedes construir un modelo CART usando la clase *DecisionTreeClassifier()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3.Clasificación No Lineal CART (Classification And Regression Trees)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"CART: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ### 4. Máquinas de Vector Soporte o Support Vector Machines (SVM):\n",
    "- Las SVM buscan la línea que mejor separa dos clases. Las instancias de datos que están más cerca de esa línea se denominan **vectores de soporte** e influyen en la ubicación de la línea. SVM también soportar **múltiples clases**. A través del parámetro *kernel* podemos utilizar diferentes funciones del kernel. Por defecto, se utiliza una potente Función de Base Radial.\n",
    "- Puedes construir un modelo SVM usando la clase *SVC()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.Clasificación No Lineal Máquina de Vector Soporte - SVM \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(gamma='scale') #podemos usar 'auto' como gamma\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"SVC: \" + str(results.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.4. Comparación y selección de modelos\n",
    "\n",
    "Es importante comparar constantemente el rendimiento de los diferentes algoritmos de aprendizaje automático según los utilizamos. \n",
    "\n",
    "Vamos a generar un marco de prueba que nos sirva como plantilla para los problemas de aprendizaje automático que tengamos, e incluso ir agregando más algoritmos diferentes que me sirvan para comparar. El objetivo:\n",
    "1. Saber cómo formular un experimento para comparar directamente los algoritmos de aprendizaje automático.\n",
    "2. Tener una plantilla reutilizable para evaluar el rendimiento de diferentes algoritmos en un conjunto de datos dado.\n",
    "3. Saber cómo informar y visualizar los resultados cuando se compara el rendimiento del algoritmo.\n",
    "\n",
    "Cuando trabajas en un proyecto de aprendizaje automático (ML), sueles acabar con varios buenos modelos para elegir. Cada uno tendrá **diferentes características de rendimiento**. \n",
    "- Con el re-muestreo con métodos como la validación cruzada, podemos obtener una estimación de la precisión de cada modelo en datos nuevos. Con estas estimaciones tienes que ser capáz de elegir 1 ó 2 de los mejores modelos del conjunto de modelos que hayamos creado.\n",
    "\n",
    "Cuando tenemos un conjunto de datos nuevo, es una buena idea visualizar los datos utilizando diferentes técnicas para tener diferentes perspectivas de los datos. Esta misma idea se aplica a la selección del model):\n",
    "- Usaremos una serie de métodos diferentes para ver la precisión estimada de los algoritmos de ML y elegir uno o dos algoritmos para ﬁnalizar. \n",
    "- Una forma de hacerlo es usar métodos de visualización que muestren la precisión media, varianza y otras propiedades de la distribución de precisiones del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compararemos los siguientes algoritmos\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Preparamos los modelos incluyendo una matriz o array con todas las soluciones\n",
    "modelos = []\n",
    "modelos.append(('LR', LogisticRegression(solver='lbfgs', max_iter=500)))\n",
    "modelos.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "modelos.append(('KNN', KNeighborsClassifier()))\n",
    "modelos.append(('CART', DecisionTreeClassifier()))\n",
    "modelos.append(('NB', GaussianNB()))\n",
    "modelos.append(('SVM', SVC(gamma='scale')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos el dataset para poder ejecutar sólo este apartado.\n",
    "filename = './pima-indians-diabetes.data.csv'\n",
    "nombres = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=nombres)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Nos generamos una plantilla en bucle con la que evaluamos cada modelo por turnos\n",
    "resultados = []\n",
    "nombres = []\n",
    "scoring = 'accuracy'\n",
    "for nombre, modelo in modelos:\n",
    "\tkfold = KFold(n_splits=10, random_state=7)\n",
    "\tcv_results = cross_val_score(modelo, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresultados.append(cv_results)\n",
    "\tnombres.append(nombre)\n",
    "\tmsg = \"%s: %f (%f)\" % (nombre, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Boxplot de comparación de algoritmos\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Comparación de los algoritmos')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(resultados)\n",
    "ax.set_xticklabels(nombres)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# 5.Mejorar la precisión del modelo\n",
    "## 5.1. Ensembles (Conjuntos)\n",
    "\n",
    "Los **3 métodos** más populares para combinar las predicciones de diferentes modelos son:\n",
    "- **Boosting**. Construye múltiples modelos (normalmente del mismo tipo), cada uno de los cuales aprende a arreglar los errores de predicción de un modelo anterior en la secuencia de modelos.\n",
    "- **Bagging**. Construye múltiples modelos (normalmente del mismo tipo) a partir de diferentes submuestras del conjunto de datos de entrenamiento.\n",
    "- **Voting (Votación)**. Construye múltiples modelos (generalmente de diferentes tipos) y estadísticas simples (como la media) se utiliza para combinar predicciones.\n",
    "\n",
    "Para revisarlos y utilizando el dataset, cada algoritmo de ensemble se trabaja utilizando una validación cruzada de 10-folds y la métrica de rendimiento de precisión (accuracy) de la clasiﬁcación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 1. Boosting.\n",
    "- Los algoritmos de **boosting** crean una secuencia de modelos que intentan corregir los errores de los modelos anteriores a ellos en la secuencia preparada. Una vez creados, los modelos hacen predicciones que pueden ser ponderadas por su precisión demostrada y los resultados se combinan para crear una predicción de salida final.\n",
    "- Lo veremos a continuación, pero a diferencia del **bagging**, en el **boosting** no se crean versiones del conjunto de entrenamiento, sino que se trabaja siempre con el conjunto completo de entrada, y se manipulan los pesos de los datos para generar modelos distintos. La idea es que en cada iteración se incremente el peso de los objetos mal clasificados por el predictor en esa iteración, por lo que en la construcción del siguiente predictor estos objetos serán más importantes y será más probable clasificarlos bien.\n",
    "- Los 2 algoritmos de boosting más comunes son:\n",
    "    1. **AdaBoost**. Fue quizás el primer algoritmo ensemble con más éxito. \n",
    "        - Por lo general, funciona ponderando las instancias en el conjunto de datos por lo fácil o difícil que es clasificarlas, lo que permite que el algoritmo les preste menos atención en la construcción de los modelos posteriores.\n",
    "        - Puede construir un modelo de AdaBoost para la clasificación utilizando la clase *AdaBoostClassifier* (en el ejemplo usaremos  30 árboles de decisión en secuencia).\n",
    "    2. **Stochastic Gradient Boosting** (Aumento de Gradiente Estocástico). \n",
    "        - El SGB (también llamado *Gradient Boosting Machine*) es una de las técnicas de ensemble más sofisticadas. También es una técnica que está demostrando ser quizás una de las mejores disponibles para mejorar el rendimiento a través de conjuntos. \n",
    "        - Utilizaremos para clasificación la clase *GradientBoostingClassifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trabajamos con los 3 métodos más populares para combinar las predicciones:\n",
    "1. Boosting. \n",
    "2. Bagging.\n",
    "3. Voting.\n",
    "\"\"\"\n",
    "#Importaciones generales para todos los ensembles\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Carga de datos y generación de los kfold\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Boosting\n",
    "    1.AdaBoost\n",
    "    2.SGB-Stochastic Gradient Boosting\n",
    "\"\"\"\n",
    "# 1.1.AdaBoost para clasificación (30 árboles)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_trees = 30\n",
    "modelo = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "resultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(\"AdaBoost: \" + str(resultados.mean()))\n",
    "\n",
    "# 1.2.SGB-Stochastic Gradient Boosting para clasificación (100 árboles)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "modelo = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "resultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(\"SGB: \" + str(resultados.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 2. Bagging o Embolsado.\n",
    "Bagging o embolsado. Se llama también Agregación (o Bagging) de Bootstrap e implica tomar múltiples muestras del conjunto de datos de entrenamiento (con reemplazo) y entrenar un modelo para cada muestra. La predicción ﬁnal es la media de las predicciones de cada uno de los submodelos.\n",
    "Vamos a ver 3 modelos de Bagging, aunque existen más:\n",
    "1. **Árboles de decisión embolsados o Bagged Decission Trees**: \n",
    "El bagging se realiza mejor con algoritmos que tienen una varianza alta. Un ejemplo típico son los árboles de decisión, a menudo construidos sin poda. Usaremos *BaggingClassifier()* con el algoritmo de árboles de clasificación y regresión (*DecisionTreeClassifier*).\n",
    "2. **Bosque Aleatorio o Random Forest**: \n",
    "**Random Forests** es una extensión de los *Bagged Decission Trees*. Las muestras del conjunto de datos de entrenamiento se toman con reemplazo, pero los árboles se construyen de una manera que reduce la correlación entre las clases individuales. Así, en vez de elegir el mejor punto de división en la construcción de cada árbol, solo se considera un subconjunto aleatorio de características para cada división. Usaremos *RandomForestClassifier()*.\n",
    "3. **Árboles adicionales o Extra Trees**: \n",
    "Es otra variación de **bagging** en la que los árboles aleatorios se construyen a partir de muestras del conjunto de datos de entrenamiento. Para crear el modelo de árboles adicionales para la clasificación usamos la clase *ExtraTreesClassifier()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Bagging (todos con 100 árboles):\n",
    "    1.Bagged Decision Trees\n",
    "    2.Random Forest\n",
    "    3.Extra Trees\n",
    "\"\"\"\n",
    "# 2.1.Bagged Decision Trees para clasificación\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "num_trees = 100\n",
    "cart = DecisionTreeClassifier()\n",
    "modelo = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "resultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(\"Bagged Decision Trees: \" + str(resultados.mean()))\n",
    "\n",
    "# 2.2.Random Forest para clasificación\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "max_features = 3\n",
    "\n",
    "modelo = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "resultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(\"Random Forest: \" + str(resultados.mean()))\n",
    "\n",
    "# Extra Trees para clasificación\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "max_features = 7\n",
    "\n",
    "modelo = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "resultados = cross_val_score(modelo, X, Y, cv=kfold)\n",
    "print(\"Extra Trees: \" + str(resultados.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">### 3. Voting o Votación.\n",
    "Es una de las formas más simples de combinar las predicciones de múltiples algoritmos de aprendizaje automático:\n",
    "- Funciona creando primero 2 o más modelos independientes a partir de su conjunto de datos de entrenamiento. Luego se puede usar un clasificador de Voting para encapsular los modelos y promediar las predicciones de los submodelos para cuando tenga que hacer predicciones para nuevos datos. \n",
    "- Las predicciones de los submodelos pueden ponderarse, pero es difícil especificar los pesos para los clasiﬁcadores de forma manual o incluso heurística.\n",
    "- Los métodos más avanzados pueden aprender cómo ponderar mejor las predicciones de los submodelos; esto se denomina apilamiento o *stacking* (agregación apilada) y actualmente no se proporciona en scikit-learn.\n",
    "- Usaremos la clase *VotingClassifier()* para crear modelos de Voting. Haremos un *ensemble* de las predicciones de regresión logística, CART y SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Voting.\n",
    "\"\"\"\n",
    "# Voting Ensemble para Clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Creamos los submodelos\n",
    "estimadores = []\n",
    "modelo1 = LogisticRegression()\n",
    "estimadores.append(('logistic', modelo1))\n",
    "modelo2 = DecisionTreeClassifier()\n",
    "estimadores.append(('cart', modelo2))\n",
    "modelo3 = SVC()\n",
    "estimadores.append(('svm', modelo3))\n",
    "\n",
    "# Creamos el modelo ensemble\n",
    "ensemble = VotingClassifier(estimadores)\n",
    "resultados = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(\"Voting Ensemble: \"+ str(resultados.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.Tuning\n",
    "- Normalmente **la validación cruzada con k-fold es el estándar de oro** para evaluar el rendimiento de un algoritmo de aprendizaje automático en datos no tratados con k establecido en 3, 5 o 10.\n",
    "- El uso de un Train-Test es bueno para **mejorar la rapidez cuando se usa un algoritmo lento** y produce estimaciones de rendimiento con **menor sesgo** cuando se usan datasets grandes.\n",
    "- Técnicas como la validación cruzada dejando uno fuera (LOO) y las divisiones aleatorias repetidas (Shuffle-splits) serán aproximaciones útiles para intenta equilibrar la varianza en el **rendimiento estimado, la velocidad de entrenamiento del modelo y el tamaño del *dataset**.\n",
    "- El mejor consejo es experimentar y encontrar una técnica rápida para el problema y que produzca estimaciones razonables de rendimiento que me permita utilizarlas para tomar decisiones. En caso de duda, utilizad **10 veces la validación cruzada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Voting Ensemble para Clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Creamos los submodelos\n",
    "estimadores = []\n",
    "modelo1 = LogisticRegression(solver='liblinear')\n",
    "estimadores.append(('logistic', modelo1))\n",
    "modelo2 = DecisionTreeClassifier()\n",
    "estimadores.append(('cart', modelo2))\n",
    "modelo3 = SVC()\n",
    "estimadores.append(('svm', modelo3))\n",
    "\n",
    "# Creamos el modelo ensemble\n",
    "ensemble = VotingClassifier(estimadores)\n",
    "resultados = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(\"Voting Ensemble: \"+ str(resultados.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# 6.Cerrar y guardar el modelo\n",
    "\n",
    "El hecho de encontrar un modelo de aprendizaje automático y afinarlo para ser lo más preciso posible no es la última etapa del proyecto.\n",
    "\n",
    "Una vez conseguido lo anterior, guardaremos nuestro modelo y podremos cargarlo a posteriori para hacer predicciones. Ahora nos quedan 2 posibles métodos:\n",
    "- Cómo usar **Pickle** para serializar y de-serializar modelos de aprendizaje automático. \n",
    "- Cómo usar **Joblib** para serializar y de-serializar modelos de aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.1.Pickle\n",
    "**Pickle**  es la forma estándar de serializar objetos en Python. \n",
    "- **Guardar**: Importaremos pickle para serializar sus algoritmos de aprendizaje automático y guardar el formato serializado en un archivo (mediante dump()). Se genera un fichero nombreModelo.sav\n",
    "- **Cargar**: En cualquier momento podemos cargar este archivo para de-serializar el modelo (mediante load()) y usarlo para hacer nuevas predicciones. \n",
    "\n",
    "En el caso de los datos de diabetes de los indios Pima, una vez entrenado el modelo de regresión logística, guardaremos el modelo en un archivo (**modelo_finalizado.sav**) y cuando recibamos nuevos conjuntos de datos podremos hacer predicciones con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# Ajustar el modelo al 33%\n",
    "modelo = LogisticRegression(solver='liblinear')\n",
    "modelo.fit(X_train, Y_train)\n",
    "\n",
    "# Guardamos el modelo al disco\n",
    "fichero_modeloPickel = 'modelo_finalizado_pickel.sav'\n",
    "dump(modelo, open(fichero_modeloPickel, 'wb'))\n",
    "\n",
    "#Cuando lo vaya a recuperar en otro momento\n",
    "\n",
    "# Cargamos el modelo del disco\n",
    "modelo_cargado = load(open(fichero_modeloPickel, 'rb'))\n",
    "resultado = modelo_cargado.score(X_test, Y_test)\n",
    "print(\"Resultado: \" + str(resultado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.2.Joblib\n",
    "La biblioteca Joblib pertenece al ecosistema SciPy y proporciona utilidades para hacer ‘pipelines’ de jobs Python. Se ayuda de estructuras de datos Numpy para guardar y cargar objetos Python, de forma muy eficiente.\n",
    "\n",
    "Con algunos algoritmos de aprendizaje automático que requieren muchos parámetros o almacenan el conjunto de datos completo (por ejemplo, K-nn) es muy útil.\n",
    "\n",
    "Al igual que en el ejemplo anterior, lo guardaremos en el fichero .sav, y una vez recuperado, lo ejecutamos y mostrará una estimación de la precisión en ese conjunto de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "\n",
    "# Guardamos el modelo al disco\n",
    "fichero_modeloJoblib = 'modelo_finalizado_joblib.sav'\n",
    "dump(modelo, fichero_modeloJoblib)\n",
    "\n",
    "#Cuando lo vaya a recuperar en otro momento\n",
    "\n",
    "# Cargamos el modelo del disco\n",
    "modelo_cargado = load(fichero_modeloJoblib)\n",
    "resultado = modelo_cargado.score(X_test, Y_test)\n",
    "print(\"Resultado: \" + str(resultado))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "## Conclusiones\n",
    "Consideraciones importantes a la hora de ﬁnalizar nuestros modelos:\n",
    "- **Versión de Python**. Anotad la versión de Python. Casi seguro que necesitaremos la misma versión (y quizás menor) de Python con la que serializamos el modelo, para hacer el proceso de carga y deserialización.\n",
    "- **Versiones de la librería**. La versión de todas las librerías principales utilizadas en el proyecto casi seguro que deberán ser las mismas cuando deserializa un modelo guardado (no solo usaremos NumPy y scikit-learn).\n",
    "- **Serialización manual**. Es posible que desee generar manualmente los parámetros de su modelo aprendido para que pueda usarlos directamente en scikit-learn (o la plataforma que uses en el futuro). \n",
    "    - A menudo, las técnicas utilizadas internamente por los algoritmos de aprendizaje automático para hacer predicciones son mucho más simples que las que se usan para aprender los parámetros y pueden ser fáciles de implementar en código personalizado más fácil de controlar por nosotros.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}