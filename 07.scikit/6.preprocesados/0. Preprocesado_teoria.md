# Preprocesado de datos

## Qué es

* [Scikit Learn API](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)

El **preprocesado de datos** es una etapa fundamental en el proceso de aprendizaje automático, en la que los datos se limpian, transforman y se les da formato para que sean adecuados para el modelo de aprendizaje automático. 

El **objetivo** del preprocesado de datos es mejorar la calidad de los datos de entrada, de tal forma que se puedan obtener resultados precisos y confiables del modelo.

Para analizar nuestros datos y extraer información de ellos, es necesario **procesar los datos** antes de comenzar a construir el modelo de aprendizaje automático.

Los datos se procesan en la forma (un formato eficiente) que el algoritmo puede interpretar fácilmente y producir el resultado requerido con precisión.

Los datos que usamos en el mundo real no son perfectos y están incompletos, inconsistentes (con valores atípicos y ruidosos) y en una forma no estructurada. 

El preprocesamiento de los datos sin procesar ayuda a organizar, escalar, limpiar (eliminar valores atípicos), estandarizar, es decir, simplificarlos para alimentar los datos al algoritmo de aprendizaje automático.

## Pasos

El proceso de preprocesamiento de datos implica algunos pasos:

1. **Limpieza de datos**: los datos que utilizamos pueden tener algunos puntos faltantes (como filas o columnas que no contienen ningún valor) o tener datos ruidosos (datos irrelevantes que son difíciles de interpretar por la máquina). Para resolver los problemas anteriores, podemos eliminar las filas y columnas vacías o llenarlas con otros valores y podemos usar métodos como la regresión y la agrupación para datos ruidosos.

2. **Selección de características**: esta técnica implica la selección de un subconjunto relevante de características del conjunto de características originales para mejorar el desempeño del modelo y reducir su complejidad. Algunas técnicas comunes incluyen la selección basada en filtro, la selección basada en envoltura y la selección basada en incrustación.

3. **Extracción de características**: esta técnica implica la creación de nuevas características a partir de las características originales utilizando técnicas de transformación matemática, reducción de dimensionalidad y clustering. Algunas técnicas comunes incluyen el Análisis de Componentes Principales (PCA), el Análisis Discriminante Lineal (LDA) y el Análisis de Clustering, Análisis de Factorización de Matrices (NMF), t-SNE y Autoencoders.

4. **Transformación de características**: esta técnica implica la transformación de las características originales en una nueva forma para mejorar el desempeño del modelo. Algunas técnicas comunes incluyen la normalización de características, la binarización de características y la discretización de características.

5. **Creación de características**: esta técnica implica la creación de nuevas características a partir de las características originales utilizando conocimientos del dominio de la tarea de aprendizaje. Algunas técnicas comunes incluyen la creación de características basadas en el conocimiento experto, la creación de características basadas en la interacción de características y la creación de características basadas en la combinación de diferentes fuentes de datos.

El objetivo de transformar los datos es el de exponer mejor la estructura del problema a los algoritmos de modelado.

No siempre es obligatorio. A veces los algoritmos pueden ofrecer mejores resultados sin procesamiento previo.

## Ventajas

El preprocesado de datos en aprendizaje automático es importante por las siguientes razones:

1.  **Mejora la calidad de los datos**: Los datos pueden contener ruido, valores faltantes, inconsistencias y otros problemas que pueden afectar la precisión del modelo. El preprocesado de datos ayuda a identificar y eliminar estos problemas, mejorando la calidad de los datos.
2.  **Permite la utilización de diferentes tipos de datos**: Los modelos de aprendizaje automático pueden requerir diferentes tipos de datos, como variables categóricas, numéricas, binarias, entre otros. El preprocesado de datos permite la conversión de los datos en un formato adecuado para el modelo.
3.  **Reduce el tiempo y costo de entrenamiento**: El preprocesado de datos reduce el tiempo y el costo de entrenamiento del modelo. Al limpiar y transformar los datos, se reducen los errores y se mejora la precisión, lo que reduce el tiempo y los recursos necesarios para entrenar el modelo.
4.  **Aumenta la precisión del modelo**: La calidad de los datos es uno de los factores más importantes que afectan la precisión del modelo. Al preprocesar los datos, se mejoran la calidad y la precisión del modelo.

## Las 4 C de la limpieza de datos
En esta etapa, limpiaremos nuestros datos:

1. **Corrigiendo** valores anómalos/erróneos y atípicos/outliers
2. **Completando** la información faltante
3. **Creando** nuevas funciones para el análisis
4. **Convirtiendo** los campos al formato correcto para los cálculos y la presentación.


### 1. Corregir

Si tenemos valores irrazonables, como por ejemplo edad = 800 en lugar de 80, entonces lo mejor es corregirlos al comienzo. 

### 2. Completar
Completar valores nulos o faltantes siempre y cuando el porcentaje de los mismos sea razonable en una columna. 

Si por ejemplo, más del 40% de datos de una feature aparecen nulos lo más probable es que descartar completamente esa feature sea la mejor opción.

### 3. Creación
Creación de nuevas features o características a partir de las anteriores de forma que puedan aportar nueva información relevante para los modelos.

### 4. Conversión
Conversión de datos categóricos a datos numéricos para poder medir correlaciones, hacer EDAs y poder usarlos en los modelos.

## Técnicas de preprocesado

### 1. Estandarizar / Escalar

* [Visualización de escalados](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)

Estandarizar significa cambiar o escalar los valores para que la desviación estándar de la distribución sea igual a uno. Generalmente significa cambiar el rango de los valores. La forma de la distribución no cambia. Piensa en cómo un modelo a escala de un edificio tiene las mismas proporciones que el original, solo que más pequeño. Por eso decimos que está dibujado a escala. El rango a menudo se establece en 0 a 1.

* ``StandardScaler``
* ``MinMaxScaler``
* ``MaxAbsScaler``
* ``RobustScaler``

**MinMaxScaler**(feature_range = (0, 1)) transformará cada valor en la columna proporcionalmente dentro del rango ``[0,1]``. Úselo como la primera opción del escalador para transformar una característica, ya que conservará la forma del conjunto de datos (sin distorsión).

**StandardScaler**() transformará cada valor de la columna en un rango de media 0 y desviación estándar 1, es decir, cada valor se normalizará restando la media y dividiendo por la desviación estándar. Utilice StandardScaler si sabe que la distribución de datos ya es normal (gaussiana). También conocido como Z-Score, convierte cada valor de una columna en un número alrededor de 0.

* El **escalado** se trata de transformar el valor de las funciones en un rango similar al de otras para que los algoritmos de aprendizaje automático se comporten mejor y den como resultado modelos óptimos. 

* El **escalado** de datos se puede lograr mediante la normalización o estandarización de variables de entrada y salida de valor real.

* El **escalado** de datos es un paso de preprocesamiento recomendado cuando se trabaja con muchos algoritmos de aprendizaje automático. No se requiere el escalado para algoritmos como el RandomForest o el árbol de decisiones.

* La **estandarización** y la **normalización** son las dos técnicas más comunes para el escalado de funciones.

* La **normalización** se trata de transformar los valores de las características para que se encuentren dentro de los intervalos acotados (mínimo y máximo)

* La **estandarización** se trata de transformar los valores de las características para que tengan una media de 0 con una desviación estándar de 1

* La **estandarización** mantiene información útil sobre los valores atípicos y hace que el algoritmo sea menos sensible a ellos en contraste con la escala min-max

* La clase **MinMaxScaler** de sklearn.preprocessing se utiliza para la normalización de características.

* La clase **StandardScaler** de sklearn.preprocessing se utiliza para la estandarización de funciones.

Se puede restaurar los datos originales a través de la función ``inverse_transform()``.



#### MinMaxScaler

 La primera opción intuitiva es usar lo que se llama el escalador Min-Max. En esto, restamos el Mínimo de todos los valores, marcando así una escala de Min a Max. 
 
 Luego divídalo por la diferencia entre Min y Max. El resultado es que nuestros valores irán de cero a 1. Esto es bastante aceptable en los casos en los que no nos preocupa la estandarización a lo largo de los ejes de varianza. p.ej. procesamiento de imágenes o redes neuronales que esperan valores entre 0 y 1.

Sin embargo, la desventaja es que debido a que ahora hemos acotado el rango de 0 a 1, tendremos desviaciones estándar más bajas y suprimirá el efecto de los valores atípicos.

La forma de superar esto es a través de Standard Scaler, o normalización de puntuación z. En primer lugar, al restar la media, trae los valores alrededor de 0, por lo que tiene una media cero. En segundo lugar, divide los valores por la desviación estándar, lo que garantiza que la distribución resultante sea estándar con una media de 0 y una desviación estándar de 1. 

Aunque la normalización a través del escalado min-max es una técnica de uso común que es útil cuando necesitamos valores en un intervalo acotado, la estandarización puede ser más práctica para muchos algoritmos de aprendizaje automático. 

La razón es que muchos modelos lineales, como la regresión logística y SVM inicializan los pesos a 0 o pequeños valores aleatorios cercanos a 0. Usando la estandarización, centramos las columnas de características en la media 0 con la desviación estándar 1, de modo que que las columnas de características toman la forma de una distribución normal, lo que facilita el aprendizaje de los pesos. Además, la estandarización mantiene información útil sobre los valores atípicos y hace que el algoritmo sea menos sensible a ellos en contraste con la escala mínima-máxima, que escala los datos a un rango limitado de valores.



#### StandardScaler

**StandardScaler** se usa cuando las características del conjunto de datos de entrada difieren mucho entre sus rangos, o simplemente cuando se miden en diferentes unidades de medida.

**StandardScaler** *elimina la media* y *escala los datos* a la varianza de la unidad. Sin embargo, los valores atípicos tienen influencia al calcular la media empírica y la desviación estándar, lo que reduce el rango de valores característicos.

Estas diferencias en las características iniciales pueden causar problemas para muchos modelos de aprendizaje automático. Por ejemplo, para modelos basados en el cálculo de la distancia, si una de las características tiene un rango de valores más amplio que otra, la distancia se regirá por esa característica en particular.

La idea detrás de StandardScaler es que las variables que se miden en diferentes escalas no contribuyen por igual al ajuste del modelo y la función de aprendizaje del modelo y podrían terminar creando un sesgo.

Entonces, para lidiar con este problema potencial, necesitamos estandarizar los datos (μ = 0, σ = 1) que se usan normalmente antes de integrarlos en el modelo de aprendizaje automático.

StandardScaler() normalizará las características, es decir, cada columna de X, INDIVIDUALMENTE, de modo que cada columna/característica/variable tendrá μ = 0 y σ = 1.

* ``μ = 0``: En caso de media cero, eso se debe a que algunos modelos de aprendizaje automático no incluyen el término de sesgo en su representación, por lo que tenemos que mover los datos alrededor del origen antes de enviarlos al algoritmo para compensar la falta de término de sesgo. 
* ``σ = 1``: En el caso de la varianza de la unidad, eso se debe a que muchos algoritmos de aprendizaje automático usan algún tipo de distancia (por ejemplo, euclidiana) para decidir o predecir. Si una característica en particular tiene valores amplios (es decir, una gran variación), la distancia se verá muy afectada por esa característica y se ignorará el efecto de otras características. Por cierto, algunos algoritmos de optimización (incluido el descenso de gradiente) tienen un mejor rendimiento cuando los datos están estandarizados.


### 2. Normalizar

La normalización es el proceso de escalar muestras individuales para tener una norma unitaria. El objetivo es cambiar los valores de las columnas numéricas en el conjunto de datos para usar una escala común, sin distorsionar las diferencias en los rangos de valores ni perder información.
* ``Normalizer``

Es decir, normalización modifica los datos de tal forma que la suma de los cuadrados de los datos queda como 1 en cada fila.


### 3. Imputación de valores faltantes

Muchos conjuntos de datos del mundo real contienen valores faltantes, a menudo codificados como espacios en blanco, NaN u otros marcadores de posición.

Tales conjuntos de datos son incompatibles con los estimadores de scikit-learn que asumen que todos los valores en una matriz son numéricos y que todos tienen y tienen un significado. 

Una estrategia básica para usar conjuntos de datos incompletos es descartar filas y/o columnas enteras que contengan valores faltantes. Sin embargo, esto tiene el precio de perder datos que pueden ser valiosos (aunque estén incompletos). 

Una mejor estrategia es imputar los valores faltantes, es decir, inferirlos de la parte conocida de los datos, asignarles un valor.

* ``SimpleImputer``
* ``IterativeImputer``
* ``KNNImputer``


### 4. Codificación de categóricos

Muchas veces los datos que utilizamos pueden no tener los valores de las características en forma continua, sino formas de categorías con etiquetas de texto. Para que el modelo de aprendizaje automático procese estos datos, es necesario convertir estas características categóricas en una forma comprensible para la máquina.

- ``OrdinalEncoder``
- ``OneHotEncoder``
- ``LabelEncoder``
- Nota: es común utilizar en su lugar la función ``pd.get_dummies`` de pandas.


**Developer Documentation:**

* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)
* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)
* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)
* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)
* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)


### 5. Discretización (Binning)

Ayuda a separar las características continuas de los datos en valores discretos (también conocido como binning o quantization). Esto es similar a crear un histograma usando datos continuos (donde la discretización se enfoca en asignar valores de características a estos contenedores). La discretización puede ayudarnos a introducir la no linealidad en modelos lineales en algunos casos. La binarización de características es el proceso de umbralizar características numéricas para obtener valores booleanos.
* ``KBinsDiscretizer``
* ``Binarizer``

[0 20000000]
[muy baratas, baratas, normales, caras, muy caras, ultra caras]

[0 100]
[jóvenes, adultos, ancianos]
[menor de edad, mayor de edad]

### 6. Transformación de datos no lineales

Permiten transformar una distribución de datos a normal (gaussiana):
* ``QuantileTransformer``
* ``PowerTransformer`` (permite las transformaciones Box-Cox y Yeo-Johnson)


### 7. Generar características polinómicas

Para obtener una mayor precisión en los resultados de nuestro modelo de aprendizaje automático, a veces es bueno introducir complejidad en el modelo (agregando no linealidad).
* ``PolynomialFeatures``
* ``SplineTransformer``


### 8. Transformaciones personalizadas

Convertir una función de Python existente en un transformador para ayudar en la limpieza o el procesamiento de datos.
* ``FunctionTransformer``


### 9. Distribución de los datos
![[sesgos.png]]

* Si la asimetría es menor que -1 o mayor que 1, la distribución es muy asimétrica.
* Si la asimetría está entre -1 y -0,5 o entre 0,5 y 1, la distribución es moderadamente asimétrica.
*  Si la asimetría está entre -0,5 y 0,5, la distribución es aproximadamente simétrica.
* Si la asimetría es 0, la distribución es simétrica

1. Datos sesgados **positivamente**:

* Transformación logarítmica (cuando los datos están muy sesgados)
	* ``log(X)`` - si no hay valores cero presentes
	* ``log(C + X)`` - si hay valores cero presentes
		* ``C`` es una constante añadida de modo que el valor más pequeño será igual a 1.

* Transformación de raíz cuadrada (cuando los datos están moderadamente sesgados)
	* ``sqrt(X)``

2. Datos sesgados **negativamente**:

* Transformación de reflejo y registro
	* ``log(K - X) - K`` es una constante de la que se restan los valores para que el valor más pequeño sea 1.
	* ``(K - X)`` hace que el número grande sea pequeño y el número pequeño grande, por lo que los datos con sesgo negativo se vuelven sesgados positivamente.

* Transformación de reflexión y raíz cuadrada
	* ``sqrt(K - X)``

Algo muy común a la hora de eliminar el sesgo en las distribuciones es: 

1. Eliminar outliers
2. Aplicar escalado, por ejemplo MinMaxScaler
3. Aplicar la transformación : logarítmica o sqrt o Box-cox o Yeo-Johnson

La transformación logarítmica es en realidad un caso especial del método Box-cox y, por lo tanto, Box-cox es un método más genérico en comparación con el método de transformación logarítmica.

Si los datos tienen valores negativos, recomendamos usar la familia de distribuciones de Yeo-Johnson que se puede usar sin restricciones en la variable de respuesta (respuesta negativa).

Para utilizar Box-cox o Yeo-Johnson se utiliza la clase de scikit-learn:

* ``PowerTransformer(method='box-cox')``
* ``PowerTransformer(method='yeo-johnson')``

### 10. Reducción de dimensionalidad (PCA)

El análisis de componentes principales (o **PCA**) utiliza el álgebra lineal para transformar el conjunto de datos en una forma comprimida. Se trata de una **técnica reducción de datos**. 

Una **propiedad** de PCA es que puede elegir el número de dimensiones o componentes principales en el resultado transformado.

### 11. Selección de características

Las características de los datos utilizados para entrenar los modelos de aprendizaje automático tienen mucha influencia en el rendimiento que darán.

Las características irrelevantes o parcialmente relevantes pueden afectar negativamente el rendimiento del modelo.

Veamos técnicas de selección automática de características que puedes utilizar para preparar sus datos de aprendizaje automático en Python con scikit-learn, destacando:

**OBJETIVO:** La selección de características es un proceso en el que selecciona automáticamente aquellas características en nuestros datos que afectan más a la variable o resultado de predicción que buscamos.

Tener características irrelevantes en los datos puede disminuir la precisión de muchos modelos, especialmente algoritmos lineales como regresión lineal y logística.

Tres beneficios de realizar la selección de características antes de modelar sus datos son:

- Reduce el **OVERFITTING** (sobreajuste): menos datos redundantes significa reducir las probabilidades de **tomar decisiones basadas en el ruido**.

- Mejora del **ACCURACY** (precisión): menos datos engañosos significa SIEMPRE una mejora del de la precisión del modelado.

- Reduce el tiempo de **TRAINING** (entrenamiento): Menos datos significa que los algoritmos se entrenan más rápido.

#### 1. Selección univariante

Jugar probando con estadísticas nos ayuda a seleccionar aquellas características que tienen una relación más fuerte con la variable de salida. 

La selección de características univariante es un método de selección de características basado en la prueba estadística univariante, por ejemplo: chi2, correlación de Pearson y muchas más.

Scikit Learn proporciona la clase [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), es un método para clasificar las características de un conjunto de datos por su "importancia" con respecto a la variable de destino. Combina la prueba estadística univariada con la selección del número K de características en función del resultado estadístico entre X e y.

Esta "importancia" se calcula mediante una función de puntuación o prueba estadística que puede ser una de las siguientes:

* ``f_classif``: valor F de ANOVA entre etiqueta/característica para tareas de clasificación
* ``f_regression``: valor F entre etiqueta/característica para tareas de regresión.
* ``chi2``: Estadísticas de chi-cuadrado de características no negativas para tareas de clasificación.
* ``mutual_info_classif``: información mutua para un objetivo discreto.
* ``mutual_info_regression``: información mutua para un objetivo continuo.
* ``SelectPercentile``: seleccione funciones según el percentil de las puntuaciones más altas.
* ``SelectFpr``: seleccione funciones en función de una prueba de tasa de falsos positivos.
* ``SelectFdr``: seleccione funciones en función de una tasa estimada de descubrimiento falso.
* ``SelectFwe``: seleccione funciones según la tasa de error familiar.
* ``GenericUnivariateSelect``: Selector de funciones univariado con modo configurable.

Todas las funciones de puntuación mencionadas anteriormente se basan en estadísticas. 

Por ejemplo, la función ``f_regression`` organiza los **valores p** (p-values) de cada una de las variables en orden creciente y selecciona las mejores **K columnas** con el menor valor p. Las características con un **p_value menor de 0,05** se consideran "significativas" y solo estas características deben usarse en el modelo predictivo.

Debido a que el método de selección de características univariado está diseñado para el aprendizaje supervisado, dividimos las características en variables independientes y dependientes. 

#### 2. Eliminación recursiva (RFE)

Recursive Feature Elimination - Eliminación Recursiva de Características

La **Eliminación Recursiva de Características** (o RFE) funciona eliminando atributos recursivamente y construyendo un modelo sobre aquellos atributos que permanecen. 

RFE es un método de selección de funciones que utiliza un modelo de aprendizaje automático para seleccionar las funciones mediante la eliminación de la función menos importante después del entrenamiento recursivo.

Según Scikit-Learn, ``RFE`` es un método para seleccionar características al considerar recursivamente conjuntos de características cada vez más pequeños. 

1. Primero, el estimador se entrena en el conjunto inicial de características, y la importancia de cada característica se obtiene a través de un atributo ``coef_`` o a través de un ``feature_importances_``. 
2. Luego, las características menos importantes se eliminan del conjunto actual de características. Ese procedimiento se repite recursivamente en el conjunto podado hasta que finalmente se alcanza el número deseado de características para seleccionar.

Este método solo funciona si el modelo tiene el atributo coef_ o features_importances_, si hay modelos que tienen estos atributos, puede aplicar RFE en Scikit-Learn. Ejemplos: regresiones (coef_) y árboles (feature_importances_).

NOTA: Este algoritmo asume que ninguna de las características está correlacionada. No es recomendable utilizar una feature si tiene un coeficiente de correlación de Pearson superior a 0,8 con cualquier otra feature.

De forma predeterminada, la cantidad de funciones seleccionadas para RFE es la mediana de las funciones totales y el paso (la cantidad de funciones eliminadas en cada iteración) es uno.


#### 3. Feature Importance (Trees)

Las bolsas de árboles de decisión (bagged decisión trees) como Random Forest y Extra Trees se pueden usar para estimar la importancia de las características.

  
Se puede construir un clasiﬁcador ``ExtraTreesClassifier`` o un regresor ``ExtraTreesRegressor`` y ver que otorga una puntuación de importancia para cada atributo en el que cuanto mayor sea ésta, más importante es el **atributo**. 


#### 4. Umbral de varianza

Una feature con una **varianza más alta** significa que el valor dentro de esa característica varía o tiene una cardinalidad alta. Por otro lado, una **varianza más baja** significa que el valor dentro de la función es similar y una varianza cero significa que tiene una función con el mismo valor.

Se desea tener una feature variada, ya que no queremos que nuestro modelo predictivo esté sesgado. Es por eso que podríamos seleccionar la característica en base a la varianza que seleccionamos previamente.

Variance Threshold o selección por umbral de varianza es un enfoque simple para eliminar características basado en nuestra varianza esperada dentro de cada característica. 

NOTA: La selección de funciones Umbral de varianza solo ve las funciones de entrada (X) sin considerar ninguna información de la variable dependiente (y). Solo es útil para eliminar funciones para el **modelado no supervisado** en lugar del modelado supervisado.


#### Conclusión

* Si el conjunto de datos no es demasiado grande, use Boruta para la selección de características.
* Si XGboost o RandomForest brindan más del 90% de precisión en el conjunto de datos, podemos usar directamente su método incorporado ".feature_importance_"
* Si solo desea la relación entre 2 variables cualesquiera y no todo el conjunto de datos en sí, es ideal optar por la puntuación p_value o la correlación de personas.


## Checklist

1. Comprobación de tipos de datos (dtypes)
2. Ajustar nombres de columnas (opcional)
3. Valores faltantes: 
	1. borrar
	2. imputar
4. Datos erróneos: por ejemplo reemplazar ``?`` por ``NaN``
5. Manipulación de datos: modificaciones necesarias
6. Outliers
7. Creación de nuevas features significativas
8. Escalado
9. Distribución de los datos
10. Desbalanceo de datos (difícil de corregir)
11. Transformaciones
12. Binning
13. Codificación de categóricos
14. Selección y reducción de características

En caso de duda, normalice los datos de entrada. Si tiene los recursos, explore el modelado con datos sin preprocesar, datos estandarizados y datos normalizados y vea si hay una diferencia beneficiosa en el rendimiento del modelo resultante.


Pasos en el preprocesado: 

1. Filas
	1. Eliminar duplicados
	2. Eliminar outliers
	3. Data Sampling: generar datos sintéticos cuando sea necesario (por ejemplo si el dataset está desbalanceado en un problema de clasificación)
2. Columnas
	1. Feature selection
	2. Polynomial features: en algoritmos como la regresión polinómica tenemos que tratar los datos antes para generar features polinómicas.
	3. One Hot Encoding
3. Valores:
	1. Imputar valores
	2. Escalar/estandarizar los datos
	3. Transformar las distribuciones
4. Columnas + Valores
	1. Reducción de la dimensionalidad


## Consejo: fit_transform

Muchos transformadores tienen la función fit_transform() y transform(). 

A la hora de aplicar una transformación sobre los datos de entrada (X) cuando hacemos particionamiento (train y test) hay que tener en cuenta:

* ``fit_transform()`` se usa en el conjunto de entrenamiento (X_train)
* ``transform()`` se usa en el conjunto de test (X_test)

Ver la explicación en detalle [aquí](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe).


