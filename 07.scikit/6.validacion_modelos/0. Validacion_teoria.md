# Validación de modelos

https://scikit-learn.org/stable/modules/cross_validation.html

https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html

Esta etapa trata de resolver la siguiente necesidad: saber cómo se comportarán nuestros algoritmos con datos aún no trabajados.

Una forma es utilizar métodos de re-muestreo, que son técnicas que permiten hacer estimaciones precisas de su comportamiento con datos nuevos. Lo vamos a hacer con métodos que tiene Python y Scikit-learn.


**PROBLEMA PLANTEADO**

*¿Cuál es el problema de usar un algoritmo de aprendizaje automático en un dataset de entrenamiento y después utilizar las predicciones de este mismo dataset para evaluar el rendimiento?* La respuesta es simple: **OVERFITTING o SOBREAJUSTE**.

Como el algoritmo se acuerda de todo lo que ha observado durante el entrenamiento y por tanto sabe la respuesta, no a nuevas preguntas, sino a las mismas de antes tendrá una puntuación perfecta con ellas. Pero cuando vaya a predecir sobre datos nuevos dará resultados muy malos.

 Utilizamos nuestro algoritmo entrenado para predecir nuevos datos (no utilizados para entrenar), con lo que la **EVALUACIÓN es una estimación para comprobar como de buenas serán las predicciones que hará en la práctica**. **NO** es una garantía de rendimiento.

Vamos a revisar **4 técnicas** que podemos usar para dividir nuestro conjunto de datos de entrenamiento y crear estimaciones útiles de rendimiento para nuestros algoritmos de aprendizaje automático:

1. Conjuntos de entrenamiento y test (particionado).

2. Validación cruzada (Cross-Validation) de k-folds.

3. Validación cruzada dejando fuera 1 partición (LOO).

4. Validación cruzada sobre particiones de Entrenamientos-Pruebas aleatorias.


### 1. Train-Test

![[cv2.png]]

Es el método más simple: utilizar **diferentes** conjuntos de datos de entrenamiento y test. Cogemos el *dataset* original y lo dividimos en 2:

- **Entrenamos** el algoritmo en la primera parte y testeamos predicciones en la segunda parte.

- **Evaluamos** las predicciones últimas contra los resultados esperados.

- **Tamaño de la división:** Puede depender del tamaño y las especificaciones del dataset, aunque es común utilizar **(2/3 – 1/3) -> el 67%** de los datos para la entrenar y el 33% restante para las pruebas, también es muy común utilizar el principio de Pareto de 80/20, siendo el 80% de los datos para entrenamiento y el 20% de los datos para test. 

**VENTAJAS:** Es muy rápido. Si hay millones de registros y hay pruebas sólidas de que cada partición sigue representando el problema subyacente, es muy útil cuando con otros métodos tarda mucho tiempo en entrenarse.

**INCONVENIENTES:** Mucha variación, que hace que las diferencias en precisión entre el dataset de entrenamiento y el de test puedan ser significativas.


### 2. Validación cruzada

![[cv1.png]]

La **validación cruzada (o Cross-Validation)** es un enfoque que puede utilizar para estimar el rendimiento de un algoritmo de AA con menos varianza que una única división de conjunto de prueba de entrenamiento.

- Dividimos el dataset en k partes (por ejemplo, k = 3, 5 ó 10 para miles de registros). Cada división de los datos se llama **split**.

- El algoritmo se entrena en k −1 splits sin usar 1 y se prueba en ese split sin usar.

- Esto se repite para que cada split del dataset en algún momento sea el no usado.

- Como resultado tenemos k diferentes valoraciones de rendimiento que podemos resumir utilizando **una media y una desviación estándar**.


![[cv-fold.png]]

  
La elección de k debe hacerse de forma que el tamaño de cada partición de prueba (test) sea lo suficientemente grande como para ser una muestra razonable del problema, a la vez que se permiten suficientes iteraciones de la evaluación del algoritmo (en Train-Test) que de buenos resultados con nuevos datos.


**VENTAJAS:**

Es más preciso porque el algoritmo se entrena y evalúa varias veces con diferentes datos.


**INCONVENIENTES:**

Depende de la selección correcta de las particiones.


### 3. Validación cruzada LOO
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html

Se trata de una variación en la que podemos configurar la validación cruzada para que el tamaño del split sea 1 (k se ajusta al número de observaciones del dataset) y se denomina **validación cruzada leave-one-out (LOO)** dejando fuera una partición.


**VENTAJA:**

Como resultado nos proporciona una gran cantidad de medidas de rendimiento resumiendo el esfuerzo necesario para ofrecer estimaciones razonables de la precisión del modelo para nuevos datos.

**INCONVENIENTE:**

Debido a la gran cantidad de conjuntos de prueba (que es la misma que la cantidad de muestras), este método de validación cruzada puede ser muy costoso.


### 4. Validación cruzada Shuffle
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit

Validación cruzada sobre particiones de Train-Test aleatorias (Shuffle Split)

Otra variación en la **validación cruzada de K-fold** es crear una división aleatoria de los datos como la división de Test-Train descrita anteriormente, pero **repetir el proceso de división y evaluación del algoritmo varias veces**, como la validación cruzada.


**VENTAJA:**

Combina la velocidad de usar una Test-Train con la reducción en la varianza en el rendimiento estimado de la validación cruzada de k-fold. Además podemos repetir el proceso tantas veces como sea necesario para mejorar la precisión.


**INCONVENIENTE:**

Las repeticiones pueden incluir gran parte de los mismos datos en los split de Train-Test de una ejecución a otra, lo que introduce redundancia en la evaluación.


### 5. Validación cruzada estratificada
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html

**Se utiliza en problemas de clasificación.**

Primero se define "KFold" y "Stratified".

* KFold es un validador cruzado que divide el conjunto de datos en k pliegues.

* Estratificar es disponer en capas. Estratificado es para garantizar que cada pliegue del conjunto de datos tenga la misma proporción de observaciones con una etiqueta determinada.

Entonces, significa que StratifiedKFold es la versión mejorada de KFold.

Se utiliza StratifiedKFold en lugar de KFold cuando se trata de tareas de clasificación con distribuciones de clases desequilibradas.

### Consejos

  
- Normalmente la **validación cruzada con k-fold es el estándar de oro** para evaluar el rendimiento de un algoritmo de aprendizaje automático en datos no tratados con k establecido en 3, 5 o 10.

- El uso de un Train-Test es bueno para **mejorar la rapidez cuando se usa un algoritmo lento** y produce estimaciones de rendimiento con **menor sesgo** cuando se usan datasets grandes.

  
- Técnicas como la validación cruzada dejando uno fuera (LOO) y las divisiones aleatorias repetidas (Shuffle-splits) serán aproximaciones útiles para intenta equilibrar la varianza en el **rendimiento estimado, la velocidad de entrenamiento del modelo y el tamaño del *dataset**.


- El mejor consejo es experimentar y encontrar una técnica rápida para el problema y que produzca estimaciones razonables de rendimiento que me permita utilizarlas para tomar decisiones. En caso de duda, utilizad 10 veces la validación cruzada.

  

### Mal uso de validación

- Usar la validación cruzada para evaluar varios modelos, y sólo indicando los resultados para el modelo con los mejores resultados.

- Realizar un análisis inicial para identificar las características más informativas utilizando el dataset completo, si la selección de característica o el ajuste del modelo lo requiere por el propio procedimiento de modelado, esto debe repetirse en cada conjunto de Train. Si se utiliza la VC para decidir qué características se van a utilizar, se deberá realizar un proceso interno de VC para llevar a cabo la selección de características en cada conjunto de Train.

- Permitir que algunos de los datos de entrenamiento esté también incluido en el conjunto de Test, esto puede suceder debido a "hermanamiento" en el dataset, con lo que varias muestras exactamente idénticas o casi idénticas pueden estar presentes en el dataset.