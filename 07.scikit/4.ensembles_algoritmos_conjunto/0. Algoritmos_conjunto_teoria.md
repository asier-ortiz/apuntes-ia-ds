# Algoritmos de conjunto (Ensembles)

**Ensemble Learning** se refiere al uso de algoritmos de ML de forma conjunta para resolver problemas de clasificación y/o regresión principalmente. 

Estos algoritmos pueden ser:
* del **mismo** tipo (aprendizaje de conjuntos *homogéneos*)
* de tipos **diferentes** (aprendizaje de conjuntos *heterogéneos*). 

Ensemble Learning realiza una combinación estratégica de varios estimadores o modelos ML con el fin de mejorar la efectividad obtenida utilizando un único modelo débil. 

Existen diferentes tipos de técnicas de Ensemble Learning que se diferencian principalmente por:

* Los tipos de modelos utilizados (modelos homogéneos o heterogéneos)
* El muestreo de datos (con o sin [reemplazo](https://www.statology.org/sampling-with-vs-without-replacement/), k-fold, etc.)
* La función de decisión (votación, promedio, meta modelo, etc.). 

Por tanto, las técnicas de Ensemble Learning se pueden clasificar en **3 categorías**:

1. **Bagging**: Construye múltiples modelos (normalmente del mismo tipo) a partir de diferentes submuestras del conjunto de datos de entrenamiento.

2. **Boosting**: Construye múltiples modelos (normalmente del mismo tipo), cada uno de los cuales aprende a arreglar los errores de predicción de un modelo anterior en la secuencia de modelos.

3. **Stacking**: Construye múltiples modelos (normalmente de diferentes tipos) y utiliza un nuevo modelo para aprender a combinar mejor las predicciones de dos o más modelos entrenados en su conjunto de datos.

De las anteriores surgen **dos variantes** importantes: 

* **Voting** (que es un complemento de *Bagging*): Construye múltiples modelos (generalmente de diferentes tipos) y estadísticas simples (como la media); se utiliza para combinar predicciones.

* **Blending** (un subtipo de *Stacking*): mismo enfoque que el *stacking*, pero utiliza solo un conjunto de reserva (validación) del conjunto de trenes para hacer predicciones. 

Aunque Voting y Blending son un complemento y un subtipo de Bagging y Stacking respectivamente, estas técnicas a menudo se encuentran como tipos directos de Ensemble Learning.


## 1. Bagging

**Bagging** o **embolsado**. Se llama también Agregación (o Bagging) de Bootstrap e implica tomar múltiples muestras del conjunto de datos de entrenamiento (con reemplazo) y entrenar un modelo para cada muestra. 

La predicción ﬁnal es la media de las predicciones de cada uno de los submodelos.

El **embolsado** consiste en entrenar varios modelos base en diferentes muestras de datos (bootstrap samples) y construir un modelo de conjunto que "promedie" los resultados de estos alumnos débiles. 

![[bagging.png]]

**Características** del bagging o embolsado:

- Los algoritmos son entrenados en paralelo mediante aleatoriedad.
- Cada modelo es entrenado de forma independiente.
- Todos los algoritmos tienen el mismo peso para la decisión final.
- Las muestras de entrenamiento son tomadas aleatoriamente del conjunto.

Vamos a ver 3 modelos de Bagging, aunque existen más:

1. **Árboles de decisión embolsados o Bagged Decision Trees**:

El bagging se realiza mejor con algoritmos que tienen una varianza alta. Un ejemplo típico son los árboles de decisión, a menudo construidos sin poda. 

* BaggingClassifier
* BaggingRegressor

2. **Bosque Aleatorio o Random Forest**:

Random Forests es una extensión de los *Bagged Decision Trees*. Las muestras del conjunto de datos de entrenamiento se toman con reemplazo, pero los árboles se construyen de una manera que reduce la correlación entre las clases individuales. 

Así, en vez de elegir el mejor punto de división en la construcción de cada árbol, solo se considera un subconjunto aleatorio de características para cada división. 

* RandomForestClassifier
* RandomForestRegressor

3. **Árboles adicionales o Extra Trees**:

Es otra variación de **bagging** en la que los árboles aleatorios se construyen a partir de muestras del conjunto de datos de entrenamiento. 

* ExtraTreesClassifier
* ExtraTreesRegressor


### Random Forest

Se les conoce como ensembles debido a que utilizan un conjunto de modelos (múltiples árboles de decisión.). Aparece en 1995.

El método de **bosque aleatorio (Random Forest)** es un método de embolsado con **árboles de decisión (Decision Trees)** como aprendices débiles. Cada árbol se ajusta a una muestra de arranque (bootstrap sample) considerando solo un subconjunto de variables/features elegidas al azar.

#### Decision Tree

La impureza Gini es la técnica más común de criterio para decisión. El problema es que solo puedes usar ciertas features, es decir, no hay garantía de usar todas las features. Por tanto estamos obligados a construir el mejor árbol de decisión basándonos en ese criterio de información. 

Para un determinado conjunto de reglas, el árbol de decisión siempre será parecido, ya que el nodo siempre será el mismo. —> El nodo raíz tiene mucha influencia sobre el árbol.

**Reglas** que podemos ajustar en el árbol de decisión:

- criterio de división
- Disminución mínima de impurezas de Gini
- Establecer límites de profundidad
- Límites en el número de nodos de hoja terminal

**Limitaciones** de los árboles de decisión:

- Solo una feature en el nodo raíz
- Los criterios de división pueden hacer que algunas features no se utilicen
- Potencial de overfitting a los datos

#### Random Forest

Mejoras que proporcionan los Random Forest:

- Crea **subconjuntos de features** elegidas al azar en cada posible split. Se elige un *subset feature number* que es el número de features a partir del cual hacer split se eligen 3 aleatoriamente. Con eso se hace un árbol de decisión el cuál elige una feature como nodo principal.

- Se crean subsets y se va pasando por todas las features.

- Al añadir el random feature subspace constraint permite que los árboles crezcan en mayor profundidad sin causar overfitting.

![[random-forest.png]]

Salidas en los Random Forest:
- **Clasificación**:
    - El resultado es que tenemos muchos árboles de decisión y se vota por la salida que más veces se repite. Se vota por mayoría.
- **Regresión**:
    - En lugar de votar por el que se repite mayoritariamente (ahora es un resultado continuo), se calcula la media de las salidas de los árboles de decisión.

**Puntos clave** de los Random Forest:
- Feature subspace
- Bootstraping: random sampling with replacement. Elegir instancias aleatoriamente  y podemos repetir una letra más de una vez.
- [Out of bag error](https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/) —> Bagging: al utilizar bootstraping para construir los árboles, puede ser que para ciertos árboles, cierta parte del dataset no se utilice en el entrenamiento. Normalmente 1/3 de los datos no son utilizados para construir cada árbol.


#### Decision Tree vs. RF

Diferencias entre árboles de decisión (CART) y árboles aleatorios (RF):

1. **Interpretabilidad**: El argumento común para usar un árbol de decisión en lugar de un Random Forest es que los árboles de decisión son más fáciles de interpretar, simplemente observa la lógica del árbol de decisión. Sin embargo, en un Random Forest, estudiar la lógica del árbol de decisión de 500 árboles diferentes es complicado.

2. **Eficiencia**: Random Forest es en realidad una colección de árboles de decisión que hace que el algoritmo sea lento e ineficaz para las predicciones en tiempo real. 
	* En general, RF puede ser rápido para entrenar, pero bastante lento para crear predicciones una vez que están entrenados. esto se debe al hecho de que tiene que ejecutar predicciones en cada árbol individual y luego promediar sus predicciones para crear la predicción final. Una solución a eso puede ser usar un clúster que ajuste diferentes árboles al mismo tiempo.
	* Una predicción más precisa requiere más árboles, lo que da como resultado un modelo más lento. En la mayoría de las aplicaciones del mundo real, el algoritmo de bosque aleatorio es lo suficientemente rápido, pero ciertamente puede haber situaciones en las que el rendimiento en tiempo de ejecución sea importante y se prefieran otros enfoques.

3. **Overfitting**: los árboles de decisión sufren de sobreajuste, mientras que Random Forest puede evitar el sobreajuste, lo que resulta en una mejor predicción la mayor parte del tiempo. Esta es una ventaja significativa del modelo de regresión Random Forest que lo hace más atractivo para muchos científicos de datos.

#### RF en Scikit Learn

* RandomForestClassifier
* RandomForestRegressor

Los parámetros entre uno y otro son casi los mismos, los nuevos que trae el RandomForest con respecto a las clases DecisionTree son:

- n_estimators, boostrap, oob_score, n_jobs, warm_start, max_samples

Los parámetros que modificamos son:

- Number of Estimators: Cuántos árboles de decisión usar?
- Number of Features: cuántas features incluir en cada subset?
- Boostrap Samples: permitir bootstrap sampling para cada subconjunto de entrenamiento de las features?
- Out-of-Bag Error (OOB): calcular el OOB durante el entrenamiento?


### Voting

El **voting** es un complemento de **Bagging**. Este tipo de conjunto es uno de los más intuitivos y fáciles de entender. 

Funciona creando primero 2 o más modelos independientes a partir de su conjunto de datos de entrenamiento. Luego se puede usar un clasificador de Voting para encapsular los modelos y promediar las predicciones de los submodelos para cuando tenga que hacer predicciones para nuevos datos.

El Clasificador de Votación es un tipo de Ensemble Learning homogéneo y heterogéneo, es decir, los clasificadores base pueden ser del mismo o diferente tipo. Como se mencionó anteriormente, este tipo de conjunto también funciona como una extensión del embolsado (por ejemplo, Random Forest).

La arquitectura de un Voting Classifier se compone de un número “n” de modelos ML, cuyas predicciones se valoran de dos formas distintas: hard y soft. En el modo difícil, la predicción ganadora es la que tiene “más votos”.

Por otro lado, el Voting Classifier en modo soft considera las probabilidades arrojadas por cada modelo de ML, estas probabilidades serán ponderadas y promediadas, en consecuencia la clase ganadora será la que tenga mayor probabilidad ponderada y promediada.

Voting es una de las formas más simples de combinar las predicciones de múltiples algoritmos de aprendizaje automático:

- Funciona creando primero 2 o más modelos independientes a partir de su conjunto de datos de entrenamiento. Luego se puede usar un clasificador de Voting para encapsular los modelos y promediar las predicciones de los submodelos para cuando tenga que hacer predicciones para nuevos datos.

- Las predicciones de los submodelos pueden ponderarse, pero es difícil especificar los pesos para los clasiﬁcadores de forma manual o incluso heurística.

- Los métodos más avanzados pueden aprender cómo ponderar mejor las predicciones de los submodelos; esto se denomina apilamiento o *stacking* (agregación apilada).

- Usaremos la clase *VotingClassifier()* para crear modelos de Voting. Haremos un *ensemble* de las predicciones de regresión logística, CART y SVM.


## 2. Boosting

**Boosting** es una técnica de meta aprendizaje que puede ser aplicado sobre cualquier algoritmo de machine learning, no únicamente árboles de decisión.

No es un algoritmo de machine learning en sí, es una forma de combinar o agregar resultados de algoritmos de machine learning, lo cual se conoce como meta aprendizaje o meta learning.

El **boosting** consiste en, iterativamente, ajustar un estimador débil, agregarlo al modelo de conjunto y "actualizar" el conjunto de datos de entrenamiento para tener mejor en cuenta las fortalezas y debilidades del modelo de conjunto actual al ajustar el siguiente modelo base.


![[boosting.png]]

**Características** del boosting:

* Los algoritmos son entrenados secuencialmente.
* Los modelos pueden tener diferente importancia o peso.
* El dataset es ponderado, de forma que aquellas observaciones que hayan sido mal clasificadas por un modelo se les da más importancia para el siguiente modelo.
* Sirven para reducir el bias y la varianza.


Los algoritmos de **boosting** crean una secuencia de modelos que intentan corregir los errores de los modelos anteriores a ellos en la secuencia preparada. Una vez creados, los modelos hacen predicciones que pueden ser ponderadas por su precisión demostrada y los resultados se combinan para crear una predicción de salida final.

![[boosting_bagging.png]]

A diferencia del **bagging**, en el **boosting** no se crean versiones del conjunto de entrenamiento, sino que se trabaja siempre con el conjunto completo de entrada, y se manipulan los pesos de los datos para generar modelos distintos. La idea es que en cada iteración se incremente el peso de los objetos mal clasificados por el predictor en esa iteración, por lo que en la construcción del siguiente predictor estos objetos serán más importantes y será más probable clasificarlos bien.

- Los 2 algoritmos de boosting más comunes son:

    1. **AdaBoost**. Fue quizás el primer algoritmo ensemble con más éxito.

        - Por lo general, funciona ponderando las instancias en el conjunto de datos por lo fácil o difícil que es clasificarlas, lo que permite que el algoritmo les preste menos atención en la construcción de los modelos posteriores.

    2. **Stochastic Gradient Boosting** (Aumento de Gradiente Estocástico).

        - El SGB (también llamado *Gradient Boosting Machine*) es una de las técnicas de ensemble más sofisticadas. También es una técnica que está demostrando ser quizás una de las mejores disponibles para mejorar el rendimiento a través de conjuntos.


### Adaboost

Utiliza weak learners, árboles de decisión con un solo nodo y dos hojas, los más simples de todos. En cada uno de ellos se evalúa una feature.

Un weak learner es un modelo que es demasiado simple para obtener buenos resultados por sí mismo.

A estos árboles de decisión débiles también se les conoce como Decision Tree Stumps. Son árboles de decisión con un nodo raíz y dos nodos hoja (el más weak de todos), donde solo 1 feature es evaluada. Esto resulta en weak learners pero que cuando se combinan crean un modelo fuerte.

En lugar de intentar encajar todo el dataset con un solo árbol (fit hard), se ensambla un modelo compuesto por múltiples árboles de decisión (weak learners) que aprenden de las features poco a poco, de manera gradual y adaptativa, primero un árbol y luego otro.
  
Es adaptativo en el sentido de que ensambla los árboles en series, desarrolla un árbol y utiliza los aprendizajes de ese árbol junto con el dataset para construir el siguiente árbol.

Utiliza pesos (estimator_weights_), de manera que cada estimator obtiene un peso en función de lo bien que ha sido capaz de predecir. El peso se establece en la fórmula matemática como el coeficiente alfa.

Cada stump representa la capacidad de una feature para predecir. Construir estos stumps en serie y utilizando un parámetro alfa permite combinar las features en base a su importancia.

  

### Gradient Boosting Machines (GBMs)

[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)

Gradient Boosting se asemeja a AdaBoost, pero todos los weak learners tienen la misma importancia ya que no se asignan pesos, se trata de predecir los errores residuales de los anteriores modelos (también es secuencial).

Usan un learning rate que toma pequeños pasos hasta obtener resultados, de forma similar a como funciona el descenso de gradiente.

Se permiten árboles más largos no solo stumps como en AdaBoost.

El coeficiente de aprendizaje es el mismo para todos los learners.

Gradient Boosting puede ser propenso a realizar overfitting ya que el número de estimators por defecto es elevado: 100.

Se optimiza una serie de árboles a través del aprendizaje de los errores residuales, forzando a los siguientes árboles a tratar de corregir el error de los anteriores.

El parámetro learning rate (learning_rate) es un número de 0 a 1. Cuanto más cercano a 0 cada árbol mejorará muy poco por lo tanto necesitará muchos árboles lo que puede conducir a mucho tiempo de entrenamiento.



## 3. Stacking

* [StackingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)
* [StackingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html)


El stacking o apilamiento es un método introducido por David H. Wolpert en 1992 donde la clave es reducir el error de generalización de diferentes generalizadores (es decir, modelos ML). 

Es una técnica de conjunto que utiliza un nuevo modelo para aprender a combinar mejor las predicciones de dos o más modelos entrenados en su conjunto de datos.

La idea general de este método es la generación de un Meta-Modelo. Dicho metamodelo se compone de las predicciones de un conjunto de modelos base de ML (es decir, estimadores débiles) a través de la técnica de validación cruzada k-fold. 

Finalmente, el metamodelo se entrena con un modelo ML adicional (que se conoce comúnmente como el "estimador final" o "aprendiz final").

![[stacking.png]]

El método de generalización de apilamiento se compone comúnmente de 2 etapas de entrenamiento, más conocidas como "nivel 0" y "nivel 1". Es importante mencionar que se pueden agregar tantos niveles como sean necesarios. Sin embargo, en la práctica es común usar solo 2 niveles. 

* **Nivel 0**: El objetivo de la primera etapa (nivel 0) es generar los datos de entrenamiento para el metamodelo, esto se lleva a cabo implementando una validación cruzada de k-fold para cada “aprendiz débil” definido en la primera etapa. Las predicciones de cada uno de estos “aprendices débiles” se “apilan” para construir dicho “nuevo conjunto de entrenamiento” (el metamodelo). 
* **Nivel 1**: El objetivo de la segunda etapa (nivel 1) es entrenar el metamodelo, dicho entrenamiento se lleva a cabo a través de un “aprendiz final” ya determinado.

### Blending

El **blending o combinación** es una técnica derivada de *Stacking* pero en lugar de ajustar el metamodelo en las predicciones fuera de las particiones (folds) realizadas por el modelo base, se ajusta a las predicciones realizadas en un conjunto de datos reservado.

También utiliza modelos base para proporcionar predicciones base como nuevas features y se entrena un nuevo metamodelo en las nuevas features que proporcionan la predicción final. 

Sigue el mismo enfoque que el *stacking*, pero utiliza solo un conjunto de reserva (validación) del conjunto de trenes para hacer predicciones. 

En otras palabras, a diferencia del *stacking*, las predicciones se realizan solo en el conjunto reservado en lugar de realizar predicciones sobre k-folds. El conjunto reservado y las predicciones se utilizan para construir un modelo que se ejecuta en el conjunto de test.


## Ensembles en Scikit Learn 
Resument de scikit [ensembles](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble).

* Bagging:
    * BaggingClassifier
    * BaggingRegressor
    * RandomForestClassifier
    * RandomForestRegressor
    * ExtraTreesClassifier
    * ExtraTreesRegressor

* Voting
	* VotingClassifier
	* VotingRegressor

* Boosting:
    * AdaBoostClassifier
    * AdaBoostRegressor
    * GradientBoostingClassifier
    * GradientBoostingRegressor

* Stacking
	* StackingClassifier
	* StackingRegressor




