{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de conjunto\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "\n",
    "* Bagging\n",
    "    * BaggingClassifier\n",
    "    * BaggingRegressor\n",
    "    * RandomForestClassifier\n",
    "    * RandomForestRegressor\n",
    "    * ExtraTreesClassifier\n",
    "    * ExtraTreesRegressor\n",
    "\n",
    "* Voting\n",
    "\t* VotingClassifier\n",
    "    * VotingRegressor\n",
    "\n",
    "* Boosting\n",
    "    * AdaBoostClassifier\n",
    "    * AdaBoostRegressor\n",
    "    * GradientBoostingClassifier\n",
    "    * GradientBoostingRegressor\n",
    "\n",
    "* Stacking\n",
    "\t* StackingClassifier\n",
    "\t* StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "df = sns.load_dataset('mpg').dropna()  # Eliminamos filas con valores nulos\n",
    "\n",
    "features = ['weight', 'cylinders', 'displacement', 'horsepower', 'acceleration', 'model_year']\n",
    "X = df[features]\n",
    "y = df['mpg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "- **Idea principal**: Entrenar varios modelos (generalmente iguales) en muestras *bootstrap* (subconjuntos del dataset con reemplazo) y luego combinar sus predicciones (votación mayoritaria o promedio).  Construye múltiples modelos a partir de diferentes submuestras del conjunto de datos de entrenamiento.\n",
    "- **Objetivo**: Reducir la varianza y mejorar la estabilidad.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `BaggingClassifier`, `BaggingRegressor`: Métodos genéricos de bagging para cualquier estimador base.\n",
    "  - `RandomForestClassifier`, `RandomForestRegressor`: Casos populares de bagging con árboles y muestreo de características.\n",
    "  - `ExtraTreesClassifier`, `ExtraTreesRegressor`: Similar a RandomForest pero con mayor aleatoriedad al dividir nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingRegressor: R2 = 0.842, MAE = 2.049, RMSE = 2.838, MAPE = 0.091\n",
      "RandomForestRegressor: R2 = 0.885, MAE = 1.761, RMSE = 2.423, MAPE = 0.080\n",
      "ExtraTreesRegressor: R2 = 0.896, MAE = 1.693, RMSE = 2.305, MAPE = 0.076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "models_bagging = {\n",
    "    \"BaggingRegressor\": BaggingRegressor(random_state=42),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_bagging.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name}: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voting\n",
    "\n",
    "- **Idea principal**: es un complemento de bagging. Combina diferentes estimadores (pueden ser modelos distintos) entrenados en el **mismo** conjunto de datos.  \n",
    "- **Cómo se combinan**:  \n",
    "  - Clasificación: votación por mayoría (hard) o promedio de probabilidades (soft).  \n",
    "  - Regresión: se hace un promedio (o combinación) de las salidas.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `VotingClassifier`  \n",
    "  - `VotingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor: R2 = 0.834, MAE = 2.119, RMSE = 2.909, MAPE = 0.096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model2 = KNeighborsRegressor()\n",
    "model3 = RandomForestRegressor(random_state=42)\n",
    "\n",
    "voting_reg = VotingRegressor([\n",
    "    (\"lr\", model1),\n",
    "    (\"knn\", model2),\n",
    "    (\"rf\", model3)\n",
    "])\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "y_pred = voting_reg.predict(X_test)\n",
    "print(f\"VotingRegressor: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "- **Idea principal**: Entrenar en secuencia múltiples “weak learners”; cada nuevo modelo se centra en corregir los errores de los modelos anteriores.  \n",
    "- **Cómo funciona**: Ajustar iterativamente modelos base (por ejemplo, árboles pequeños) dando más peso a los errores o los residuos en cada paso.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `AdaBoostClassifier`, `AdaBoostRegressor`: Ajusta pesos de los ejemplos para corregir errores.  \n",
    "  - `GradientBoostingClassifier`, `GradientBoostingRegressor`: Ajuste secuencial basado en el gradiente de la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostRegressor: R2 = 0.818, MAE = 2.240, RMSE = 3.048, MAPE = 0.103\n",
      "GradientBoostingRegressor: R2 = 0.872, MAE = 1.822, RMSE = 2.551, MAPE = 0.082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "models_boosting = {\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=42),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_boosting.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{name}: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stacking (Stacked Generalization)\n",
    "\n",
    "- **Idea principal**: Entrenar varios modelos base y luego usar **sus predicciones** como entradas para un modelo “meta” que aprende la mejor forma de combinarlas.  \n",
    "- **Cómo funciona**: \n",
    "  1. Entrenar los modelos base.  \n",
    "  2. Usar sus predicciones para alimentar un segundo modelo.  \n",
    "  3. El metamodelo combina los outputs de los anteriores.  \n",
    "- **Ejemplos en Scikit-Learn**:\n",
    "  - `StackingClassifier`  \n",
    "  - `StackingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingRegressor: R2 = 0.853, MAE = 1.952, RMSE = 2.737, MAPE = 0.086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "base_estimators = [\n",
    "    (\"lr\", LinearRegression()),\n",
    "    (\"dt\", DecisionTreeRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "model = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=RandomForestRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"StackingRegressor: R2 = {r2_score(y_test, y_pred):.3f}, MAE = {mean_absolute_error(y_test, y_pred):.3f}, RMSE = {root_mean_squared_error(y_test, y_pred):.3f}, MAPE = {mean_absolute_percentage_error(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
